<TITLE> week235 </TITLE>




<P>
After leaving the Perimeter Institute near the end of June, 
I went home to Riverside and then took off for a summer in Shanghai.  
That's where I am now.  I'm having a great time - you can read 
about it in my 
<A HREF = "http://math.ucr.edu/home/baez/diary/july_2006.html#july5.06">online 
diary</A>!

<P>
Today I'll talk about classical and quantum computation, then 
quantum gravity, and finally a bit about higher gauge theory.

<P>
My interest in quantum computation was revived when Scott
Aaronson invited me to this building near the Perimeter
Institute:

<P>
1) Institute for Quantum Computing (IQC), 
<A HREF = "http://www.iqc.ca/">http://www.iqc.ca/</A>

<P>
Raymond Laflamme gave me a fun tour of the labs, especially the 
setup where he's using nuclear magnetic resonance to control 
the spins of three carbon-13 nuclei in a substance called malonic
acid.  Each molecule is its own little quantum computer:

<P>
<div align = center>
<img src = "malonic_acid.jpg">
</div>

<P>
This picture was drawn by Osama Moussa, a grad student working with
Laflamme on this project.  The black spheres are carbons, the whites 
are hydrogens and the blues are oxygens.

<P>
One of the banes of quantum computation is &quot;decoherence&quot;, in
which the quantum state of the computer interacts with its
environment and becomes correlated with it, or &quot;entangled&quot;,
in a way that appears to &quot;collapse its wavefunction&quot; and ruin
the calculation.

<P>
In general it's good to keep things cool if you don't want 
things to get messed up.  Surprisingly, Laflamme said that 
for liquids, keeping them hot <em>reduces</em> the rate of decoherence:
the different molecules zip around so fast they
don't stay near each other long enough to affect each other much!  

<P>
But malonic acid is a solid, so the main way to keep the
molecules from disturbing each other is to keep them far apart
on average.  So, they dilute the malonic acid
made using carbon-13 nuclei in a lot of ordinary malonic acid made using 
carbon-12.  Since 12 is an even number, such carbon atoms have
no unpaired spinning neutron in their nuclei.  So, the ordinary
malonic acid serves as an inert &quot;shield&quot; that keeps the 
active molecules well separated most of the time.

<P>
So, each active molecule acts like an isolated system, doing its own 
computation as they zap it with carefully timed pulses of 
microwaves and the three spinning nuclei interact.  About a quadrillion of 
these molecules are doing their thing in parallel, mixed
in with a bunch more made using carbon-12.  For more details, see:

<P>
2) Jonathan Baugh, Osama Moussa, Colm A. Ryan, Raymond Laflamme, 
Chandrasekhar Ramanathan, Timothy F. Havel and David G. Cory,
Solid-state NMR three-qubit homonuclear system for quantum information 
processing: control and characterization, Phys. Rev. A 73 (2006), 022305. 
Also available as <A HREF = "http://arxiv.org/abs/quant-ph/0510115">
quant-ph/0510115</A>.

<P>
Laflamme also showed me some beams of spin-entangled photons which
they can use as keys for quantum cryptography.  Nobody can peek
at these photons without affecting them!  It's a great scheme.
If you don't know it, try this simple explanation:

<P>
3) Artur Ekert, Cracking codes, part II, Plus Magazine,
<A HREF = "http://pass.maths.org.uk/issue35/features/ekert/index.html">
http://pass.maths.org.uk/issue35/features/ekert/index.html</A>

<P>
There are already two companies - idQuantique and MagiQ - selling 
quantum key distribution systems that send entangled photons 
down optical fibers.  But the folks at the IQC are planning to
send them right through the air!

<P>
Eventually they want to send them from satellites down to the 
Earth.  But as a warmup, they'll send beams of entangled photons 
from an intermediate building to the Institute of Quantum Computing 
and the Perimeter Institute.

<P>
<DIV ALIGN = CENTER>
<A HREF = "http://www.iqc.ca/laboratories/peg/free_space.php">
<IMG WIDTH = 400 SRC = "cryptomap.gif">
</A>
</DIV>
<P>

<P>
4) IQC, Free-space quantum key distribution, 
<A HREF = "http://www.iqc.ca/laboratories/peg/free_space.php">http://www.iqc.ca/laboratories/peg/free_space.php</A>

<P>
Then they can share secrets with 
nobody able to spy on them unnoticed.  They should do something to 
dramatize this capability!  Unfortunately they don't actually <em>have</em> 
any secrets.  So, they might need to make some up.

<P>
The really cool part, though, is that Scott helped me see that
at least in principle, quantum computers could keep from drifting
off course without the computation getting ruined by quantum 
entanglement with the environment.  I had long been worried about 
this.  

<P>
You see, to make any physical system keep acting &quot;digital&quot; for a
long time, one needs a method to keep its time evolution from 
drifting off course.  It's easiest to think about this issue 
for an old-fashioned, purely classical digital computer.  It's 
already an interesting problem.  

<P>
What does it mean for a physical system to act &quot;digital&quot;?  Well,
we like to idealize our computers as having a finite set of states;
with each tick of the clock it jumps from one state to another in 
a deterministic way.  That's how we imagine a digital computer.

<P>
But if our computer is actually a machine following the laws of 
classical mechanics, its space of states is actually continuous - 
and time evolution is continuous too!  Physicists call the space 
of states of a classical system its &quot;phase space&quot;, and they describe 
time evolution by a &quot;flow&quot; on this phase space: states move 
continuously around as time passes, following Hamilton's equations.

<P>
So, what we like to idealize as a single state of our classical
computer is actually a big bunch of states: a blob in phase space,
or &quot;macrostate&quot; in physics jargon.  

<P>
For example, in our idealized description, we might say a wire
represents either a 0 or 1 depending on whether current is 
flowing through it or not.  But in reality, there's a blob of 
states where only a little current is flowing through, and 
another blob of states where a lot is flowing through.  All 
the former states count as the &quot;0&quot; macrostate in our idealized 
description; all the latter count as the &quot;1&quot; macrostate.  

<P>
Unfortunately, there are also states right on the brink, where a 
medium amount of current is flowing through!  If our machine gets
into one of these states, it won't act like the perfect digital
computer it's trying to mimic.  This is bad!

<P>
So, you should imagine the phase space of our computer as having
a finite set of blobs in it - macrostates where it's doing something
good - separated by a no-man's land of states where it's not 
doing anything good.  For a simple 2-bit computer, you can imagine
4 blobs like this:

<PRE>
              --------------------------
             |??????????????????????????|
             |??? ----- ?????? ----- ???|
             |???|     |??????|     |???|
             |???| 0 0 |??????| 0 1 |???|
             |???|     |??????|     |???|
             |??? ----- ?????? ----- ???|
             |??????????????????????????|
             |??? ----- ?????? ----- ???|
             |???|     |??????|     |???|
             |???| 1 0 |??????| 1 1 |???|
             |???|     |??????|     |???|
             |??? ----- ?????? ----- ???|
             |??????????????????????????|
              --------------------------
</PRE>
though in reality the phase space won't be 2-dimensional, but 
instead much higher-dimensional.

<P>
Now, as time evolves for one tick of our computer's clock, we'd 
like these nice macrostates to flow into each other.  Unfortunately, 
as they evolve, they sort of spread out.  Their volume doesn't change - 
this was shown by Liouville back in the 1800s:

<P>
5) Wikipedia, Liouville's theorem (Hamiltonian),
<A HREF = "http://en.wikipedia.org/wiki/Liouville's_theorem_(Hamiltonian)">
http://en.wikipedia.org/wiki/Liouville's_theorem_(Hamiltonian)</A>

<P>
But, they get stretched in some directions and squashed in others.  
So, it seems hard for each one to get mapped completely into another, 
without their edges falling into the dangerous no-man's-land 
labelled &quot;???&quot;.

<P>
We want to keep our macrostates from getting hopelessly smeared 
out.  It's a bit like herding a bunch of sheep that are drifting 
apart, getting them back into a tightly packed flock.  Unfortunately, 
Liouville's theorem says you can't really &quot;squeeze down&quot; a flock of 
states!  Volume in phase space is conserved....

<P>
So, the trick is to squeeze our flock of states in some directions 
while letting them spread out in other, irrelevant directions.   

<P>
The relevant directions say whether some bit in memory is a zero or 
one - or more generally, anything that affects our computation.  The 
irrelevant ones say how the molecules in our computer are wiggling 
around... or the molecules of air <em>around</em> the computer - or anything
that doesn't affect our computation.

<P>
So, for our computer to keep acting digital, it should pump out 
<em>heat!</em>  

<P>
Here's a simpler example.  Take a ball bearing and drop it into
a wine glass.  Regardless of its initial position and velocity - 
within reason - the ball winds up motionless at the bottom of 
the glass.  Lots of different states seem to be converging to 
one state!  

<P>
But this isn't really true.  In fact, information about the ball's 
position and velocity has been converted into <em>heat</em>: irrelevant 
information about the motion of atoms.

<P>
In short: for a fundamentally analogue physical system to keep 
acting digital, it must dispose of irrelevant information, which 
amounts to pumping out waste heat.  

<P>
In fact, Rolf Landauer showed back in 1961 that getting rid of 
one bit of information requires putting out this much energy 
in the form of heat:

<P>
kT ln(2)

<P>
where T is the temperature and k is Boltzmann's constant.  That's
not much - about 3 x 10<sup>-21</sup> joules at room temperature!  But,
it's theoretically important.

<P>
What had me worried was how this would work for quantum computation.
A bunch of things are different, but some should be the same.  When 
we pump information - i.e., waste heat - from the computer into 
the environment, we inevitably correlate its state with that of the 
environment.  

<P>
In quantum mechanics, correlations often take the form of
&quot;entanglement&quot;.  And this is a dangerous thing.  For
example, if our quantum computer is in a superposition of lots of
states where it's doing interesting things, and we peek at it to see
<em>which</em>, we get entangled with it, and its state seems to
&quot;collapse&quot; down to one specific possibility.  We say it
&quot;decoheres&quot;.
 
<P>
Won't the entanglement caused by pumping out waste heat screw up
the coherence needed for quantum computation to work its wonders?

<P>
I finally realized the answer was: maybe not.  Yes, the quantum state 
of the computer gets entangled with that of the environment - but 
maybe if one is clever, only the <em>irrelevant</em> aspects of its state 
will get entangled: aspects that don't affect the computation.  After 
all, it's this irrelevant information that one is trying to pump out, 
not the relevant information.

<P>
So, maybe it can work.  I need to catch up on what people have written
about this, even though most of it speaks the language of &quot;error
correction&quot; rather than thermodynamics.  Here are some things,
including material Scott Aaronson recommended to me.

<P>
Gentle introductions:

<P>
6) Michael A. Nielsen and Isaac L. Chuang, Quantum Computation and 
Quantum Information, Cambridge University Press, Cambridge, 2000.
 
<P>
7) John Preskill, Quantum computation - lecture notes, references
etc. at <A HREF = "http://www.theory.caltech.edu/people/preskill/ph229/">http://www.theory.caltech.edu/people/preskill/ph229/</A>

<P>
8) John Preskill, Fault-tolerant quantum computation, to appear in
&quot;Introduction to Quantum Computation&quot;, eds. H.-K. Lo,
S. Popescu, and T. P. Spiller.  Also available as <A HREF =
"http://arxiv.org/abs/quant-ph/9712048">quant-ph/9712048</A>.

<P>
Chapter 7 of Preskill's lecture notes is about error correction.

<P>
This is a nice early paper on getting quantum computers to work
despite some inaccuracy and decoherence:

<P>
9) Peter Shor, Fault-tolerant quantum computation, 37th Symposium on
Foundations of Computing, IEEE Computer Society Press, 1996,
pp. 56-65.  Also available as <A HREF =
"http://arxiv.org/abs/quant-ph/9605011">quant-ph/9605011</A>.

<P>
This more recent paper shows that in a certain model, quantum 
computation can be made robust against errors that occur at less
than some constant rate:

<P>
10) Dorit Aharonov and Michael Ben-Or, Fault-tolerant quantum
computation with constant error rate, available as <A HREF =
"http://arxiv.org/abs/quant-ph/9906129">quant-ph/9906129</A>.

<P>
Here's a paper that assumes a more general model:

<P>
11) Barbara M. Terhal and Guido Burkard, Fault-tolerant quantum
computation for local non-markovian noise, Phys. Rev. A 71, 012336
(2005).  Also available as <A HREF =
"http://arxiv.org/abs/quant-ph/0402104">quant-ph/0402104</A>.

<P>
Rolf Landauer was a physicist at IBM, and he discovered the result
mentioned above - the &quot;thermodynamic cost of forgetting&quot; -
in a study of Maxwell's demon.  This is a fascinating and
controversial subject, and you can learn more about it in this book of
reprints:

<P>
12) H. S. Leff and Andrew F. Rex, editors, Maxwell's Demon: Entropy,
Information and Computing, Institute of Physics Publishing, 1990.

<P>
I think Landauer's original paper is in here.  He figured out why
you can't get free energy from heat by using a little demon to
watch the molecules and open a door to let the hot ones into a
little box.  The reason is that it takes energy for the demon to
forget what it's seen!  

<P>
Finally, on a somewhat different note, if you just want a great
read on the interface between physics and computation, you've got
to try this:

<P>
13) Scott Aaronson, NP-complete problems and physical reality, ACM
SIGACT News, March 2005.  Also available as <A HREF =
"http://arxiv.org/abs/quant-ph/0502072">quant-ph/0502072</A>.

<P>
Can a soap film efficiently solve the traveling salesman problem
by minimizing its area?  If quantum mechanics were slightly nonlinear,
could quantum computers solve NP problems in polynomial time?  And
what could quantum <em>gravity</em> computers do?  Read and learn and the
state of the art on puzzles like these.

<P>
At the Perimeter Institute I also had some great discussions with
Laurent Freidel and his student Aristide Baratin.  They have a new
spin foam model that reproduces ordinary quantum field theory - in
other words, particle physics in flat spacetime.  It's not interesting
as a model of quantum gravity - it doesn't include gravity!  Instead,
it serves as a convenient target for spin foam models that <em>do</em>
include gravity: it should be the limit of any such model as the
gravitational constant approaches zero.

<P>
14) Aristide Baratin and Laurent Freidel, Hidden quantum gravity
in 4d Feynman diagrams: emergence of spin foams.  Available as
<A HREF = "http://arxiv.org/abs/hep-th/0611042">hep-th/0611042</a>.

<P>
It's the sequel of this paper for 3d spacetime:

<P>
15) Aristide Baratin and Laurent Freidel, Hidden quantum gravity in 3d
Feynman diagrams.  Available as <A HREF =
"http://arxiv.org/abs/gr-qc/0604016">gr-qc/0604016</A>.

<P>
Freidel, Kowalski-Glikman and Starodubtsev have also just come out with 
a paper carrying out some of the exciting project I mentioned in 
&quot;<A HREF = "week208.html">week208</A>&quot;:

<P>
16) Laurent Freidel, Jerzy Kowalski-Glikman and Artem Starodubtsev,
Particles as Wilson lines in the gravitational field, available 
as <A HREF = "http://arxiv.org/abs/gr-qc/0607014">gr-qc/0607014</A>.  

<P>
Their work is based on the MacDowell-Mansouri formulation of gravity.
This is a gauge theory with gauge group SO(4,1) - the symmetry group 
of deSitter spacetime.  DeSitter spacetime is a lot like Minkowski 
spacetime, but it has constant curvature instead of being flat.  
It's really just a hyperboloid in 5 dimensions:

<P>
{(w,x,y,z,t) :  w<sup>2</sup> + x<sup>2</sup> + y<sup>2</sup> + z<sup>2</sup> - t<sup>2</sup> = k<sup>2</sup>}

<P>
for some constant k. It describes an exponentially expanding 
universe, a lot like ours today.  It's the most symmetrical 
solution of Einstein's equation with a positive cosmological 
constant.  The cosmological constant is proportional to 1/k<sup>2</sup>.

<P>
When you let the cosmological constant approach zero, which is the 
same as letting k &rarr; &infin;, DeSitter spacetime flattens out to 
Minkowski spacetime, and the group SO(4,1) contracts to the symmetry 
group of Minkowski spacetime: the Poincare group.  

<P>
So, MacDowell-Mansouri gravity is similar to the formulation of
gravity as gauge theory with the Poincare group as gauge group.  I
explained that pretty carefully back in &quot;<A HREF =
"week176.html">week176</A>&quot;.

<P>
But, there's one way SO(4,1) is better than the Poincare group.  
It's a &quot;simple&quot; Lie group, so it has an inner product on its Lie 
algebra that's invariant under conjugation.  This lets us write down 
the BF Lagrangian:

<P>
tr(B ^ F)

<P>
where tr is defined using the inner product, F is the curvature of 
an SO(4,1) connection A, and B is an so(4,1)-valued 2-form.  Spin 
foam models of BF theory work really well:

<P>
17) John Baez, An introduction to spin foam models of BF theory and 
quantum gravity, in Geometry and Quantum Physics, eds. Helmut 
Gausterer and Harald Grosse, Lecture Notes in Physics, 
Springer-Verlag, Berlin, 2000, pp. 25-93.  Also available as 
<A HREF = "http://arxiv.org/abs/gr-qc/9905087">gr-qc/9905087</A>.

<P>
So, the MacDowell-Mansouri approach is a natural for spin foam
models.  It's not that MacDowell-Mansouri gravity <em>is</em> a BF theory -
but its Lagrangian is the BF Lagrangian plus extra terms.  So, 
we can think of it as a perturbed version of BF theory.  

<P>
There's also one way SO(4,1) is worse than the Poincare group.  
It's a simple Lie group - so it doesn't have a god-given 
&quot;translation&quot; subgroup the way the Poincare group does.  The 
Poincare gauge theory formulation of general relativity requires 
that we treat translations differently from boosts and rotations.  
We can't do this in an SO(4,1) gauge theory unless we break the 
symmetry down to a smaller group: the Lorentz group, SO(3,1).

<P>
So, to get MacDowell-Mansouri gravity from SO(4,1) BF theory, 
we need to add extra terms to the Lagrangian that break the 
symmetry group down to SO(3,1).  This isn't bad, just a bit sneaky.

<P>
The new paper by Freidel, Kowalski-Glikman and Starodubtsev is 
mainly about the SO(4,1) BF theory rather than full-fledged
MacDowell-Mansouri gravity.  They show that if you cut out
curves in spacetime and couple them to the A field in the right 
way, they act like the worldlines of point particles.  In particular, 
they have a mass and spin, and they trace out geodesics when their 
spin is zero.  Spinning particles do something a bit fancier, but it's 
the right thing.

<P>
This generalizes some results for 3d gravity that I explained in
detail back in &quot;<A HREF = "week232.html">week232</A>&quot;.  It's
nice to see it working in 4 dimensions too.

<P>
Back then I also explained something else about 4d BF theory: if you
cut out <em>surfaces</em> in spacetime and couple them to the
<em>B</em> field, they act like the worldsheets of 1-dimensional
extended objects, which one might call <em>strings</em>.  I don't
think they're the wiggling stretchy strings that string theorists
like; I think their equation of motion is different.  But I should
actually check!  It's stupid; I should have checked this a long time
ago.

<P>
Ahem.  Anyway, it's really neat how particles couple to the 
A field and &quot;strings&quot; couple to the B field in BF theory.  

<P>
This is vaguely reminiscent of how the A and B field form two 
parts of a &quot;2-connection&quot; - a gadget that lets you define 
parallel transport along curved and surfaces.  You can read 
about 2-connections here:

<P>
18) John Baez and Urs Schreiber, Higher gauge theory, to appear in the
volume honoring Ross Street's 60th birthday, available as <A HREF =
"http://arxiv.org/abs/math.DG/0511710">math.DG/0511710</A>.

<P>
The cool thing is that a pair consisting of an A field and a
B field gives well-behaved parallel transport for curves and
surfaces only if they satisfy an equation... which is <em>implied</em>
by the basic equation of BF theory!  

<P>
The above paper is a summary of results without proofs.  Before one
can talk about 2-connections, one needs to understand 2-bundles, which
are a &quot;categorified&quot; sort of bundle where the fiber is not a
smooth manifold but a smooth category.  My student Toby Bartels
recently finished writing an excellent thesis that defines 2-bundles
and relates them to &quot;gerbes&quot; - another popular approach to
higher gauge theory, based on categorifying the concept of
&quot;sheaf&quot; instead of &quot;bundle&quot;:

<P>
19) Toby Bartels, Higher Gauge Theory I: 2-bundles, available 
as <A HREF = "http://arxiv.org/abs/math.CT/0410328">math.CT/0410328</A>.

<P>
The detailed study of 2-connections will show up in the next
installment - a paper I'm writing with Urs Schreiber.  

<P>
You can also see transparencies of some talks about this 
stuff:

<P>
20) John Baez, Alissa Crans and Danny Stevenson, Chicago
lectures on higher gauge theory, available at 
<A HREF = "http://math.ucr.edu/home/baez/namboodiri/">http://math.ucr.edu/home/baez/namboodiri/</A>

<P>
21) John Baez, Higher gauge theory, 2006 Barrett lectures, available
at <A HREF =
"http://math.ucr.edu/home/baez/barrett/">http://math.ucr.edu/home/baez/barrett/</A>

<P>
It'll be lots of fun if higher gauge theory and the work
relating MacDowell-Mansouri gravity to BF theory fit together 
and develop in some nontrivial direction.  But the funny thing
is, I don't how they fit together yet.  

<P>
Here's why.  In gauge theory, there's a famous way to get a number
from a connection A and a loop.  First you take the
&quot;holonomy&quot; of A around the loop, and then you take the trace
(in some representation of your gauge group) to get a number.  This
number is called a &quot;Wilson loop&quot;.

<P>
This is an obvious way to define an <em>action</em> for a particle coupled 
to a connection A - at least if the particle moves around a loop.
For example, it's this action that let us compute knot invariants 
from BF theory: you use the BF action for your fields, you use the
Wilson loop as an action for your particle, and you compute the 
amplitude for your particle to trace out some knot in spacetime.

<P>
One might guess from the title &quot;Particles as Wilson lines in the 
gravitational field&quot; that this is the action Freidel and company use.  
But it's not!

<P>
Instead, they use a different action, which involves extra fields on
the particle's worldline, describing its position and momentum.  I
explained a close relative of this action back in &quot;<A HREF =
"week232.html">week232</A>&quot;, when I was coupling particles to 3d
gravity.

<P>
The same funny difference shows up when we couple strings to the B
field.  In higher gauge theory you can define holonomies and Wilson
loops using the A field, but you can also define
&quot;2-holonomies&quot; and &quot;Wilson surfaces&quot; using both
the A and B fields.  The 2-holonomy describes how a string changes as
it moves along a surface, just as the holonomy describes how a
particle changes as it moves along a curve.  If you have a closed
surface you can take a &quot;trace&quot; of the 2-holonomy and get a
number, which deserves to be called a &quot;Wilson surface&quot;.

<P>
This is an obvious way to define an action for a string coupled to 
the A and B fields - at least if it traces out a closed surface.  
But, it's not the one Perez and I use!  Why not?  Because we were
trying to do something analogous to what people did for particles
in 3d gravity.

<P>
So, there's some relation between this &quot;particles and strings
coupled to 4d BF theory&quot; business and the mathematics of higher
gauge theory, but it's not the obvious one you might have guessed
at first.

<P>
Mysteries breed mysteries.  For more musings on these topics, 
try my talk at the Perimeter Institute:

<P>
22) John Baez, Higher-dimensional algebra: a language for quantum
spacetime, colloquium talk at Perimeter Institute,
available at <A HREF = "http://math.ucr.edu/home/baez/quantum_spacetime/">http://math.ucr.edu/home/baez/quantum_spacetime/</A>

<P>
<HR>
<b>Addenda:</b> I thank Chris Weed for catching some errors, Osama
Moussa for letting me display his picture and for
catching some more errors, and Ben Rubiak-Gould, Nathan Urban and 
K. Eric Drexler for some interesting comments.
Here's my reply to Ben Rudiak-Gould:

<blockquote>
John Baez wrote:

<PRE>
  In fact, Rolf Landauer showed back in 1961 that getting rid of 
  one bit of information requires putting out this much energy 
  in the form of heat:
  
  kT ln(2)
</PRE>

Ben Rubiak-Gould replied:

<PRE>
 It's easy to understand where this formula comes from.  
 Getting rid of a bit means emitting one bit of entropy, 
 which is k ln 2 in conventional units.  The associated 
 quantity of heat is ST = kT ln 2.
</PRE>

Thanks; I should have said that.  

<P>
 Landauer's analysis showing that &quot;forgetting information&quot; costs
 energy is still interesting, and it was surprising at the time.  
 There had been a number of other analyses of why Maxwell's demon
 can't get you something for nothing, by Szilard and others, but
 none (I think) had focussed on the key importance of resetting
 the demon's memory to its initial state.

<PRE>
 But it seems to me that you're conflating two different 
 issues here.  One is the cost of forgetting a bit, which 
 only affects irreversible computation, and the other is 
 the cost of keeping the computation on track, which affects 
 reversible computation also.  Landauer's formula tells you 
 the former, but I don't think there's any lower bound on 
 the latter.
</PRE>
 Not in principle: with a perfectly tuned dynamics, an analogue 
 system can act perfectly digital, since each macrostate gets mapped
 perfectly into another one with each click of the clock.  But with
 imperfect dynamics, dissipation is needed to squeeze each macrostate
 down enough so it can get mapped into the next - and the dissipation 
 makes the dynamics irreversible, so we have to pay a thermodynamic cost.  
 
<P>
 If I were smarter I could prove an inequality relating the &quot;imperfection 
 of the dynamics&quot; (how to quantify that?) to the thermodynamic cost
 of computation, piggybacking off Landauer's formula.  
</blockquote>

Here's what Nathan Urban wrote:

<blockquote>
John Baez wrote:

<PRE>
 [quantum computation]
 So, maybe it can work.  I need to catch up on what people 
 have written about this, even though most of it speaks the 
 language of &quot;error correction&quot; rather than thermodynamics.
</PRE>

A nice recent overview of some of this work can be found in the 
latest <em>Physics Today</em>:

<P>
23) Sarma, Freedman, and Nayak, Topological quantum
computation, Physics Today (July 2006).

<P>
In this approach, error-free computation is accomplished using
topological quantum field theories, as topological theories are robust
against local perturbations.

<P>
The article has some nice discussion of anyons, braidings, non-Abelian
topological phases of condensed matter systems, etc.  It speculates
that the &nu; = 12/5 state of the fractional quantum Hall effect might
support universal topological quantum computation (meaning that its
braiding operators could realize any desired unitary transformation).
</blockquote>

<P>
Here's my reply:

<blockquote>
Long time no see, Nathan!

<P>
Nathan Urban wrote:

<PRE>
 John Baez wrote:

  >[quantum computation]
  >So, maybe it can work.  I need to catch up on what people 
  >have written about this, even though most of it speaks the 
  >language of &quot;error correction&quot; rather than thermodynamics.

 A nice recent overview of some of this work can be found in the 
 latest <em>Physics Today</em> (July 2006), in the article &quot;Topological 
 quantum computation&quot; by Sarma, Freedman, and Nayak.  (Nayak is 
 at UCLA if you ever get out that way.)
</PRE>

<P>
Thanks, I'll check that out.

<P>
I'm usually too lazy to drive into LA, but now that I'm in Shanghai,
I thought I'd take the chance to visit Zhenghan Wang in the nearby 
city of Hangzhou and talk to him about topological quantum computation.

<P>
Wang and Freedman both work for &quot;Project Q&quot;, aptly named after the 
Star Trek villain - it's Microsoft's project to develop quantum 
computers using nonabelian anyons:

<P>
24) Topological quantum computing at Indiana University, 
<A HREF = "http://www.tqc.iu.edu/">http://www.tqc.iu.edu/</A>

<PRE>
 The article has some nice discussion of anyons, braidings, 
 non-Abelian topological phases of condensed matter systems, 
 etc.  It speculates that the &nu;=12/5 state of the fractional 
 quantum Hall effect might support universal topological 
 quantum computation (meaning that its braiding operators 
 could realize any desired unitary transformation).
</PRE>

Freedman, Larsen and Wang have already proved that certain versions of 
Chern-Simons theory support universal quantum computation:

<P>
25)  Michael Freedman, Michael Larsen, and Zhenghan Wang, A 
modular functor which is universal for quantum computation, 
available as <A HREF = "http://arxiv.org/abs/quant-ph/0001108">quant-ph/0001108</A>.

<P>
The fractional quantum Hall effect is supposedly described by 
Chern-Simons theory, so this is relevant.   I don't know anything 
about the "&nu; = 12/5 state" of the fractional quantum Hall effect, 
but the folks at Project Q <em>do</em> want to use the fractional quantum 
Hall effect for quantum computation, and some people are looking 
for nonabelian anyons in the &nu; = 5/2 state:

<P>
26) Parsa Bonderson, Alexei Kitaev and Kirill Shtengel, Detecting
non-abelian statistics in the &nu; = 5/2 fractional quantum Hall
state, Phys. Rev. Lett. 96 (2006) 016803.  Also available as <A HREF =
"http://arxiv.org/abs/cond-mat/0508616">cond-mat/0508616</A>.

<P>
Apparently there's just one lab in the world that has the capability
of producing these fractional quantum Hall states!
</blockquote>

The article in the latest Physics Today isn't free for nonsubscribers,
but this is, and it seems to cover similar ground:

<P>
27) Charles Day, Devices based on the fractional quantum Hall effect may
fulfill the promise of quantum computing, Physics Today (October 2005),
also available at <A HREF = "http://www.physicstoday.org/vol-58/iss-10/p21.html">http://www.physicstoday.org/vol-58/iss-10/p21.html</A>

<P>
It discusses both the &nu; = 12/5 and &nu; = 5/2 states.

<P>
Alas, I never managed to visit Zhenghan Wang in Hangzhou.

<P>
K. Eric Drexler writes:

<blockquote>
Dear John,
<P>

To continue a thread in Week 235:

<P>
John Baez wrote:

<PRE>
 > [...] with a perfectly tuned dynamics, an analogue system 
 > can act perfectly digital, since each macrostate gets 
 > mapped perfectly into another one with each click of 
 > the clock.  But with imperfect dynamics, dissipation 
 > is needed to squeeze each macrostate down enough so it 
 > can get mapped into the next - and the dissipation 
 > makes the dynamics irreversible, so we have to pay a 
 > thermodynamic cost.  
</PRE>

Logically reversible computation can, in fact, be kept on track  
without expending energy and without accurately tuned dynamics. A  
logically reversible computation can be embodied in a constraint  
system resembling a puzzle with sliding, interlocking pieces, in  
which all configurations accessible from a given input state  
correspond to admissible states of the computation along an oriented  
path to the output configuration. The computation is kept on track by  
the contact forces that constrain the motion of the sliding pieces.  
The computational state is then like a ball rolling along a deep  
trough; an error would correspond to the ball jumping out of the  
trough, but the energy barrier can be made high enough to make the  
error rate negligible. Bounded sideways motion (that is, motion in  
computationally irrelevant degrees of freedom) is acceptable and  
inevitable.

<P>
Keeping a computation of this sort on track clearly requires no  
energy expenditure, but moving the computational state in a preferred  
direction (forward!) is another matter. This requires a driving  
force, and in physically realistic systems, this force will be  
resisted by a "friction" caused by imperfections in dynamics that  
couple motion along the progress coordinate to motion in other,  
computationally irrelevant degrees of freedom. In a broad class of  
physically realistic systems, this friction scales like viscous drag:  
the magnitude of the mean force is proportional to speed, hence  
energy dissipation per distance travelled (equivalently, dissipation  
per logic operation) approaches zero as the speed approaches zero.

<P>
Thus, the thermodynamic cost of keeping a classical computation free  
of errors can be zero, and the thermodynamic cost per operation of a  
logically reversible computation can approach zero. Only Landauer's 
ln(2)kT cost of bit erasure is unavoidable, and the number of bits  
erased is a measure of how far a computation deviates from logical  
reversibility. These results are well-known from the literature, and  
are important in understanding what can be done with atomically-precise 
systems.

<P>
With best wishes, <br>
<P>
Eric
</blockquote>

For an introduction to Drexler's plans for atomically-precise
reversible computers, see:

<P>
28) K. Eric Drexler, Nanosystems: Molecular Machinery, Manufacturing, 
and Computation, John Wiley and Sons, New York, 1992.

<P>
The issue of heat dissipation in such devices is also studied here:

<P>
29) Ralph C. Merkle, Two types of mechanical reversible logic,
Nanotechnology 4 (1993), 114-131.  Also available at 
<A HREF = "http://www.zyvex.com/nanotech/mechano.html">
http://www.zyvex.com/nanotech/mechano.html</A>

<P>
I need to think about this stuff more!




<P>
<HR>
<P>

