<TITLE> week287 </TITLE>

<!-- BEGIN HEADER -->
<TABLE WIDTH = 100%> <TR>
<TD WIDTH=10%>
<A HREF = "week286.html">
   <img border = none; src="lastweek.png"></A>
<TD WIDTH=80%>
<CENTER>
<A HREF="README.html">
  <img border = none; src="home.png"><br>
</A>
<A HREF="http://math.ucr.edu/home/baez/TWF.html">
    <img border = none; src="contents.png">
</A>
</CENTER>
<TD WIDTH=10%>
<A HREF = "week288.html">
  <img border = none; src="nextweek.png">
</A>
</TABLE>
<H4> December 19, 2009 </H4>
<H2> This Week's Finds in Mathematical Physics (Week 287) </H2>
<H4> John Baez </H4>
<!-- END HEADER -->

<P>
This week: a fascinating history of categorical logic, and more about
rational homotopy theory.  But first, guess what this is a picture of:

<p>
<div align = "center">
<img border = "2" src = "014379_0925.jpg">
</div>

<p>
If you give up, go to the bottom of this article.

<p>
Next, here's an incredibly readable introduction to the revolution
that happened in logic starting in the 1960s:

<p>
1) Jean-Pierre Marquis and Gonzalo Reyes, The history of categorical
logic, 1963-1977.  Available at
<a href = "https://www.webdepot.umontreal.ca/Usagers/marquisj/MonDepotPublic/HistofCatLog.pdf">https://www.webdepot.umontreal.ca/Usagers/marquisj/MonDepotPublic/HistofCatLog.pdf</a>

<p>
It's a meaty but still bite-sized 116 pages.  It starts with the
definitions of categories, functors, and adjoint functors.  But it
really takes off in 1963 with Bill Lawvere's thesis, which
revolutionized universal algebra using category theory.  It then moves
on through Lawvere and Tierney's introduction of the modern concept of
topos, and it ends in 1977, when Makkai and Reyes published their book
on categorical logic, and Johnstone published his book on topos theory.
The world has never been the same since!

<p>
One great thing about this paper is that it discusses the history in a
blow-by-blow way, including conferences and unpublished but
influential writings.  It also gives a great summary of the key ideas
in Lawvere's thesis.  I'll quote it, since everyone should know or at
least <i>have seen</i> these ideas:

<p>
<blockquote>
   1. To use the category of categories as a framework for mathematics,
   i.e. the category of categories should be the foundations of
   mathematics;

<p>
   2. Every aspect of mathematics should be representable in one way or
   another in that framework; in other words, categories constitute the
   background to mathematical thinking in the sense that, in this
   framework, essential features of that thinking are revealed;

<p>
   3. Mathematical objects and mathematical constructions should be
   thought of as functors in that framework;

<p>
   4. In particular, sets always appear in a category, there are no
   such thing as sets by themselves, in fact there is no such thing as
   a mathematical concept by itself;

<p>
   5. But sets form categories and the latter categories play a key role
   in the category of categories, i.e. in mathematics;

<p>
   6. Adjoint functors occupy a key position in mathematics and in the
   development of mathematics; one of the guiding principles of the
   development of mathematics should be &quot;look for adjoints to given
   functors&quot;; in that way foundational studies are directly linked to
   mathematical practice and the distinction between foundational
   studies and mathematical studies is a matter of degree and
   direction, it is not a qualitative distinction;

<p>
   7. As the foregoing quote clearly indicates, Lawvere is going back
   to the claim made by Eilenberg and Mac Lane that the &quot;invariant
   character of a mathematical discipline can be formulated in these
   terms&quot; [i.e. in terms of functoriality]. But now, in order to
   reveal this invariant character, extensive use of adjoint functors
   is made. 

<p>
   8. The invariant content of a mathematical theory is the
   &quot;objective&quot; content of that theory; this is expressed at various
   moments throughout his publications.  To wit: 

<blockquote>
      As posets often need to be deepened to categories to accurately
      reflect the content of thought, so should inverses, in the sense
      of group theory, often be replaced by adjoints. Adjoints retain
      the virtue of being uniquely determined reversal attempts, and 
      very often exist when inverses do not.
</blockquote>

<p>
   9. Not only sets should be treated in a categorical framework, but
   also logical aspects of the foundations of mathematics should be
   treated categorically, in as much as they have an objective
   content. In particular, the logical and the foundational are
   directly revealed by adjoint functors.
</blockquote>

<p>
If this sounds mysterious, well, read the paper!

<p>
Now I want to dig a little deeper into rational homotopy theory,
and start explaining this chart:

<PRE>
                      RATIONAL SPACES
                         /      \  
                        /        \  
                       /          \  
                      /            \
                     /              \
      DIFFERENTIAL GRADED ------- DIFFERENTIAL GRADED
      COMMUTATIVE ALGEBRAS           LIE ALGEBRAS
      
</PRE>
Last time I described rational spaces and the category they form: the
&quot;rational homotopy category&quot;.  I actually described this category in
three ways.  But there are two other ways to think about this category
that are much cooler!

<p>
That's what the other corners of this triangle are.  The lower left
corner involves a drastic generalization of differential forms on
smooth manifolds.  The lower right corner involves a drastic
generalization of Lie algebras coming from Lie groups.

<p>
Today I'll explain the road to the lower left corner.  Very roughly,
it goes like this.  If you give me a space, I can replace it with a
space made of simplices that has the same homotopy type, and then take
the differential forms on this replacement.  Voil&agrave;!  A
differential graded commutative algebra!

<p>
But I want to avoid talking to just the experts.  Among other things,
I want to use rational homotopy theory as an excuse to explain lots of
good basic math.  So, first I'll remind you about differential forms
on a manifold, and why they're a &quot;differential graded commutative
algebra&quot;, or &quot;DGCA&quot; for short.  Then I'll show you how
to define something like differential forms starting from any
topological space!  Again, they're a DGCA.  And it turns out that for
rational spaces, this DGCA knows everything there is to know about our
space - at least as far as homotopy theory is concerned.

<p> 
So: what are differential forms?  Differential forms are a basic
tool for doing calculus on manifolds.  We use them throughout physics:
they're the grownup version of the &quot;gradient&quot;,
&quot;divergence&quot; and &quot;curl&quot; that we learn about as
kids.  There are lots of ways to define them, but the most rapid is
this.

<p>
The smooth real-valued functions

<p>
f: X &rarr; R

<p>
on a manifold X form an algebra over the real numbers.  In other
words: you can add and multiply them, and multiply them by real
numbers, and a bunch of familiar identities hold, which are the axioms
for an algebra.  Moreover, this algebra is commutative:

<p>
fg = gf

<p>
Starting from this commutative algebra, or any commutative algebra A,
we can define differential forms as follows.  First let's define
vector fields in a purely algebraic way.  Since the job of a vector
field is to differentiate functions, people call them 
<a href = "http://en.wikipedia.org/wiki/Derivation_%28abstract_algebra%29">derivations</a>
in this algebraic approach.  A "derivation of A" is a linear map

<p>
v: A &rarr; A

<p>
which obeys the product rule

<p>
v(ab) = v(a)b + av(b)

<p>
Let Der(A) be the set of derivations of A.  This is a "module" of the
algebra A, since we can multiply a derivation by a guy in A and get a
new derivation.  (This part works only because A is commutative.)

<p>
Next let's define 1-forms.  Since the job of these is to eat vector
fields and spit out functions, let's define a "1-form" to be a linear 
map

<p>
&omega;: Der(A) &rarr; A

<p>
which is actually a module homomorphism, meaning

<p>
&omega;(fv) = f &omega;(v)

<p>
whenever f is in A.  Let &Omega;<sup>1</sup>(A) be the set of 1-forms.  Again,
this is a module of A.  

<p>
Just as you'd expect, there's a map 

<p>
d: A &rarr; &Omega;<sup>1</sup>(A)

<p>
defined by 

<p>
(df)(v) = v(f)

<p>
and you can check that

<p>
d(fg) = (df) g + f dg

<p>
So, we've got vector fields and 1-forms!  It's a bit tricky, but you
can prove that when A is the algebra of smooth real-valued functions
on a manifold, the definitions I just gave are equivalent to all the
usual ways of defining vector fields and 1-forms.  One advantage
of working algebraically is that we can generalize.  For example, we
can take A to consist of <em>polynomial</em> functions.  We'll use this
feature in a minute.

<p>
But what about other differential forms?  There's more to life than
1-forms: there are p-forms for p = 0,1,2,...  

<p>
To get these, we just form the <a href =
"http://en.wikipedia.org/wiki/Exterior_algebra">exterior algebra</a>
of the module &Omega;<sup>1</sup>(A).  You may have seen the exterior
algebra of a vector space - if not, it may be hard understanding the
stuff I'm explaining now.  The exterior algebra of a module over a
commutative algebra works the same way!  To build it, we run around
adding and multiplying guys in A and &Omega;<sup>1</sup>(A), all the
while making sure to impose the axioms of an <a href =
"http://en.wikipedia.org/wiki/Associative_algebra">associative unital
algebra</a>, together with these rules:

<p>
f (dg) = (dg) f

<p>
(df) (dg) = - (dg) (df)

<p>
The stuff we get forms an algebra: the algebra of "differential forms"
for A, which I'll call &Omega;(A).  And when A is the smooth functions
on a manifold, these are the usual differential forms that everyone
talks about!

<p>
Now, thanks to the funny rule 

<p>
(df) (dg) = - (dg) (df)

<p>
the algebra &Omega;(A) is not commutative.  However, it's &quot;graded
commutative&quot;, meaning roughly that it's commutative except for some
systematically chosen minus signs.

<p>
A bit more precisely: every differential form can be written as a
linear combination of guys like this:

<p>
v = f dg<sub>1</sub> dg<sub>2</sub> &hellip; dg<sub>p</sub>

<p>
where p ranges over all natural numbers.  Linear combinations of guys
of this sort for a particular fixed p are called &quot;p-forms&quot;.
We also say they're &quot;of degree p&quot;.  And the algebra of
differential forms obeys

<p>
&nu;&omega; = (-1)<sup>pq</sup> &omega;&nu;

<p>
whenever &nu; is of degree p and &omega; is of degree q.  This is what we
mean by saying &Omega;(A) is &quot;graded commutative&quot;.

<p>
But the algebra of differential forms is better than a mere graded
commutative algebra!  We've already introduced df when f is an element
of our original algebra.  But we can define "d" for <i>all</i>
differential forms simply by saying that d is linear and saying that d
of

<p>
f dg<sub>1</sub> dg<sub>2</sub> ... dg<sub>p</sub>

<p>
is 

<p>
df dg<sub>1</sub> dg<sub>2</sub> ... dg<sub>p</sub>

<p>
This definition implies three facts.  First, it implies that d of a
p-form is a (p+1)-form.  That's pretty obvious.  Second, it implies
that

<p>
d(d&omega;) = 0 

<p>
for any differential form &omega;.  Why?  Well, I'll let you check it, but
I'll give you a hint: the key step is to show that d1 = 0.  And third,
it implies this version of the product rule:

<p>
d(&nu;&omega;) = (d&nu;) &omega; + (-1)<sup>p</sup> &nu;d&omega; 

<p>
for any p-form &nu; and q-form &omega;.  Again the proof is a little
calculation.

<p>
We can summarize these three facts, together with the linearity of d, by
saying that differential forms are a &quot;differential graded commutative
algebra&quot;, or &quot;DGCA&quot;.  

<p>
You can do lots of wonderful stuff with differential forms.  After you
learn a bunch of this stuff, it becomes obvious that you should
generalize them to apply to spaces of many kinds.  

<p>
It's easy to generalize them from manifolds to spaces X where you 
have a reasonable idea of when a real-valued function

<p>
f: X &rarr; R

<p>
counts as &quot;smooth&quot;.  Just take the commutative algebra A of smooth
real-valued functions on X and construct &Omega;(A) following my
instructions!

<p>
There are many examples of such spaces, including manifolds with
boundary, manifolds with corners, and infinite-dimensional manifolds.
In fact, there are general theories of &quot;smooth spaces&quot; that
systematically handle lots of these examples:

<p>
2) Andrew Stacey, Comparative smootheology, available as
<a href = "http://arxiv.org/abs/0802.2225">arXiv:0802.2225</a>.

<p>
3) Patrick Iglesias-Zemmour, Diffeology.  Available at
<a href = "http://math.huji.ac.il/~piz/Site/The%20Book/The%20Book.html">http://math.huji.ac.il/~piz/Site/The%20Book/The%20Book.html</a>

<p>
4) John Baez and Alexander Hoffnung, Convenient categories of smooth
spaces, to appear in Trans. Amer. Math. Soc..  Also available as
<a href = "http://arxiv.org/abs/0807.1704">arXiv:0807.1704</a>.

<p>
But here's a question that sounds harder: how can we generalize differential
forms to an arbitrary <i>topological</i> space X?

<p>
You could take A to be the algebra of <i>continuous</i> functions on X and
form &Omega;(A).  There's no law against it... go ahead... but I bet
no good will come of it.  (What goes wrong?)

<p>
But there's a better approach, invented by Dennis Sullivan in this
famous paper:

<p>
5) Dennis Sullivan, Infinitesimal computations in topology,
Publications Mathimatiques de l'IHES 47 (1977), 269-331.  Available
at <a href = "http://www.numdam.org/item?id=PMIHES_1977__47__269_0">http://www.numdam.org/item?id=PMIHES_1977__47__269_0</a>

<p>
We start by turning our topological space into a simplicial set.  
Remember, a simplicial set is a bunch of

<p>
0-simplices (vertices)<br/>
1-simplices (edges)<br/>
2-simplices (triangles)<br/>
3-simplices (tetrahedra)<br/>

<p>
and so on, all stuck together.  Given a topological space X, we can 
form an enormous simplicial set whose n-simplices are all possible
maps 

<p>
f: &Delta;<sup>n</sup> &rarr; X

<p>
where &Delta;<sup>n</sup> is the standard n-simplex, that is, the intersection
of the hyperplane 

<p>
x<sub>0</sub> + x<sub>1</sub> + ... + x<sub>n</sub> = 1

<p>
with the set where all the coordinates x<sub>i</sub> are nonnegative.

<p>
This enormous simplicial set is called the &quot;singular nerve&quot;
of X, Sing(X).  Like any simplicial set, we can think of Sing(X) as a
purely combinatorial gadget, but we can also &quot;geometrically
realize&quot; it and think of it as a topological space in its own
right.  The resulting space is called |Sing(X)|.

<p>
(For more details on the singular nerve and its geometric realization,
see items E and F of &quot;<a href = "week116.html">week116</A>&quot;.)

<p>
This space |Sing(X)| is made of a bunch of simplices stuck together
along their faces.  So, we can say a real-valued function on |Sing(X)|
is &quot;simplex-wise smooth&quot; if it's continuous and smooth on each
simplex.  And this is enough to set up a theory of differential forms!
We just take the algebra A of simplex-wise smooth functions on
|Sing(X)|, and use this to build our algebra of differential forms
&Omega;(A) as I've described!

<p>
But Sullivan noted that we can go even further.  Thanks to how we've
defined the standard n-simplex, it makes sense to talk about
polynomial functions on this simplex.  We can even sidestep the need
for real numbers, by looking at polynomial functions with <i>rational</i>
coefficients.  And that's just right for rational homotopy theory.

<p>
So, let's focus our attention on functions on |Sing(X)| that when
restricted to any simplex give polynomials with rational coefficients.
This is a commutative algebra over the <i>rational</i> numbers.  Call it A.
We can copy our previous construction of &Omega;(A) but now working with 
rational numbers instead of reals.  Let's call guys in here &quot;rational 
differential forms&quot;.

<p>
Now, you may complain that we're not really studying differential
forms on X: we're studying them on this other space |Sing(X)|.  At one
point in my life this really annoyed me.  It seemed like a cheat.  But
for the purposes of homotopy theory it's perfectly fine, since
|Sing(X)| has the same homotopy type as X.  

<P>
(By this, I really mean they're isomorphic in the &quot;homotopy
category&quot;, which I defined last week.  So: they're the
same, as far as homotopy theory is concerned.)

<p>
And even better, when X is a rational space, the rational differential
forms on |Sing(X)| will know <i>everything</i> about the homotopy type of
X.  This is amazing!  It means that for rational spaces, we can reduce
homotopy theory to a souped-up version of the theory of differential
forms!

<p>
In particular, Sullivan was able to use this trick to compute 
the <i>homotopy groups</i> of a rational space X, starting from the
rational differential forms on |Sing(X)|.  

<p>
Since X and |Sing(X)| have the same homotopy type, they have the same
homotopy groups, and cohomology groups, and so on.  And it's not
surprising that we can read off the <i>cohomology groups</i> of
|Sing(X)| starting from the rational differential forms on this space
- this is just a slight twist on the usual idea of deRham cohomology.
But it's surprising that we can compute the <i>homotopy groups</i>, which
are usually a lot harder.  This is the magic of rational homotopy
theory.  

<p>
I won't explain this magic, at least not today.  For that read
Sullivan's paper, or this paper I recommended last time:

<p>
6) Kathryn Hess, Rational homotopy theory: a brief introduction,
in Interactions Between Homotopy Theory and Algebra, ed. 
Luchezar L. Avramov, Contemp. Math 436, AMS, Providence, Rhode
Island, 2007.   Also available as <a href = "http://arxiv.org/abs/math.AT/0604626">math.AT/0604626</a>.

<p>
For more detail, try this book:

<p>
7) Phillip A. Griffiths and John W. Morgan, Rational Homotopy 
Theory and Differential Forms, Birkh&auml;user, Boston, 1981.

<p>
Someday I should explain exactly the sense in which (certain) DGCAs
are &quot;the same&quot; as rational homotopy types.  But not today!

<p>
Instead, I want to go over what I just said in a slightly more formal
way.  This will give me an excuse to introduce a bunch of beautiful
concepts that everyone should know... and maybe demonstrate a tiny bit
of what Lawvere was talking about: the power of categories.

<p>
First of all, what's a DGCA, really?  It's a commutative monoid in the
symmetric monoidal category of cochain complexes!

<p>
Let me explain.

<p>
A &quot;cochain complex&quot;, for us, will be a list of vector spaces and
linear maps

<PRE>
    d       d      d
C<sub>0</sub> ---&gt; C<sub>1</sub> ---&gt; C<sub>2</sub> ---&gt; ...
</PRE>
with d<sup>2</sup> = 0.  We can use vector spaces over any field we
like; let's use the rational numbers to be specific.

<p>
Just as you can tensor vector spaces, you can tensor cochain complexes.
The tensor product of cochain complexes C and C' will have

<p>
(C &otimes; C')<sub>n</sub> = &oplus<sub><sub>p+q = n</sub></sub> 
C<sub>p</sub> &otimes; C'<sub>q</sub>

<p>
and we define

<p>
d(c &otimes; c') = dc &otimes; c' + (-1)<sup>p</sup> c &otimes; dc' 

<p>
when c is in C<sub>p</sub> and c' is in C'<sub>q</sub>.  

<p>
(You've seen a similar &quot;product rule&quot; earlier in this
article.  There's a general principle at work here.  Physicists know
that whenever you exchange two fermions, their phase gets multiplied
by -1.  In math, we should stick in a minus sign whenever we switch
two &quot;odd&quot; things.  The map d counts as odd since it sends
guys in our cochain complex to guys whose degree is 1 more, and the
number 1 is odd.  The element c in C<sub>p</sub> counts as
&quot;odd&quot; whenever p is odd.  In the equation above, we're
switching d past c and getting a minus sign whenever c is odd.)

<p>
Just as you can define a commutative algebra to be a vector space
V with a product

<p>
V &otimes; V &rarr; V

<p>
that's associative and commutative, you can define a &quot;differential
graded commutative algebra&quot;, or DGCA, to be a cochain complex C with a
product

<p>
C &otimes; C &rarr; C

<p>
that's associative and graded-commutative.  By &quot;graded-commutative&quot;, 
I mean you need to remember to put in a sign (-1)<sup>pq</sup> whenever you
switch a guy in C<sup>p</sup> and a guy in C<sup>q</sup>.

<p>
We can systematize all this by checking that, just like the category
of vector spaces with its usual tensor product, the category of
cochain complexes with its tensor product is a &quot;symmetric monoidal
category&quot;:

<p>
8) nLab, Symmetric monoidal category, <a href =
"http://ncatlab.org/nlab/show/symmetric+monoidal+category">http://ncatlab.org/nlab/show/symmetric+monoidal+category</a>

<p>
So is the category of sets with its cartesian product.  We can define
a &quot;commutative monoid&quot; in any symmetric monoidal category.   In the
category of sets, this is just a commutative monoid in the traditional
sense.  In the category of vector spaces, it's a commutative algebra.
And in the category of cochain complexes, it's a DGCA!

<p>
Notice: a DGCA where only C<sub>0</sub> is nonzero is just a plain old
commutative algebra.  So, DGCAs really are a generalization of
commutative algebras.  So whenever anyone tells you something about
DGCAs, you should check to see what it says about commutative
algebras.  And whenever anyone tells you something about commutative
algebras, you should try to generalize it to DGCAs!

<p>
This should keep you pretty busy, since commutative algebras are the
playground of the simplest kind of algebraic geometry: the kind where
you look at solutions of polynomial equations in a bunch of variables,
like this:

<p>
x<sup>2</sup> + y<sup>2</sup> + z<sup>2</sup> = 0<br/>
xyz - 1 = 0

<p>
If you take your polynomials and count them as zero when they satisfy
your equations, you get a commutative algebra.  Even better, you can
get any sufficiently small commutative algebra this way - the
technical term is &quot;finitely presented&quot;.  And if you allow infinitely
many variables and infinitely many equations, you can drop that
technical fine print.

<p>
So, the study of commutative algebras is really just the study of
polynomial equations.  And if we think about their solutions as
forming curves or surfaces or the like, we're doing algebraic geometry
- so-called &quot;affine&quot; algebraic geometry.

<p>
This means that we can - and in fact should! - generalize all of
affine algebraic geometry from commutative algebras to DGCAs.  I'd 
like to say more about this someday... but not today.  This is just
a digression.  I got distracted from my real goal.

<p>
Before I got distracted, I was telling you how commutative algebras
are the same as DGCA's with only C<sub>0</sub> being nonzero.  And
here's why I mentioned this.  We can take <i>any</i> DGCA and
violently kill C<sub>p</sub> for all p &gt; 0, leaving a commutative
algebra C<sub>0</sub>.  We can think of this as a forgetful functor

<p>
[DGCAs] &rarr; [commutative algebras]

<p>
And this functor has a left adjoint, which freely generates a DGCA
starting from a commutative algebra:

<p>
[commutative algebras] &rarr; [DGCAs]

<p>
Now, I've already told you about process that takes a commutative 
algebra and creates the DGCA.  Namely, the process that takes a
commutative algebra A and gives the DGCA of differential forms, 
&Omega;(A).  So, you might think this left adjoint is just that!

<p>
I thought so too, when I was first writing this.  But it turns out not
to be true - at least not always!  The left adjoint gives a slightly
<em>different</em> kind of differential forms for our commutative
algebra A.  Let's call these the &quot;K&auml;hler forms&quot;
&Omega;<sub>K</sub>(A).

<p>
The K&auml;hler 1-forms are usually called "<a href =
"http://en.wikipedia.org/wiki/K%C3%A4hler_differential">K&auml;hler
differentials</a>".  We can can build them as follows: take the
A-module generated by symbols

<p>
df
<p>
which obey the 3 basic relations we expect in calculus:

<p>
d(cf) = c df

<p>
d(f + g) = df + dg

<p>
d(fg) = f dg + (df) g

<p>
where f,g are in A and c is in our field.  This gives the A-module of
K&auml;hler differentials - let's call this
&Omega;<sub>K</sub><sup>1</sup>(A).  The K&auml;hler forms
&Omega;<sub>K</sub>(A) are then the exterior algebra on
&Omega;<sub>K</sub><sup>1</sup>(A).

<p>
By how we've set things up, the K&auml;hler differentials 
are blessed with a map

<p>
d: A  &rarr; &Omega;<sub>K</sub><sup>1</sup>(A)<br/>

<p>
And this map is a "derivation",
meaning it satisfies the 3 rules listed above.
But here's the cool part: the K&auml;hler differentials are the
<em>universal</em> A-module with a derivation.  In other words, suppose
M is any A-module equipped with a map

<p>
v: A &rarr; M

<p>
that's a derivation in the above sense.  Then there's a unique
A-module homomorphism

<p>
j: &Omega;<sub>K</sub><sup>1</sup>(A) &rarr; M

<p>
such that

<p>
v = j d

<p>
The proof is easy: just define j(df) = v(f) and check that everything
works!

<p>
Thanks to this universal property, K&auml;hler differentials are much
beloved by algebraists.  So, it's natural to wonder if they're the
same as the 1-forms &Omega;<sup>1</sup>(A) that I explained above!

<p>
As it turns out, these 1-forms are the double dual of the K&auml;hler
differentials:

<p>
&Omega;<sup>1</sup>(A) = &Omega;<sub>K</sub><sup>1</sup>(A)**

<p>
Sometimes we get 

<p>
&Omega;<sup>1</sup>(A) = &Omega;<sub>K</sub><sup>1</sup>(A)

<p>
and this case it's easy to check that

<p>
&Omega;(A) = &Omega;<sub>K</sub>(A)

<p>
But sometimes the 1-forms and the K&auml;hler differentials are
<em>different</em>.  Let me explain why.  It's technical, but fun
if you're already familiar with some of these ideas.

<p>
For starters, let me explain what I mean!  We've got a commutative
algebra A.  If we have an A-module M, its "dual" M* is the set of all
A-module maps

<p>
w: M &rarr; A

<p>
The dual becomes a module in its own right by

<p>
(gw)(f) = g w(f)

<p>
So, we can take the dual of the dual, M**.  And then there's always a
module homomorphism

<p>
j: M &rarr; M**

<p>
given by

<p>
j(f)(w) = w(f)

<p>
for f in M, w in M*.  Sometimes j is an isomorphism: for example, when
M is <a href = 
"http://en.wikipedia.org/wiki/Finitely_generated_module">finitely 
generated</a> and <a href = 
"http://en.wikipedia.org/wiki/Projective_module">projective</a>.  But often 
it's not.  And that's where the subleties arise.

<p>
If you look back at my definition of 1-forms, it amounted to this:

<p>
&Omega;<sup>1</sup>(A) = Der(A)*

<p>
And the universal property of K&auml;hler differentials gives us this:

<p>
Der(A) &cong; &Omega;<sub>K</sub><sup>1</sup>(A)*

<p>
Putting these facts together, we get

<p>
&Omega;<sup>1</sup>(A) &cong; &Omega;<sub>K</sub><sup>1</sup>(A)**

<p>
So, we always have a module homomorphism 

<p>
j: &Omega;<sub>K</sub><sup>1</sup>(A) &rarr; &Omega;<sup>1</sup>(A)

<p>
This is <em>both</em> the map we always get from a module to its
double dual, <em>and</em> the map we get from the universal property of
K&auml;hler differentials.

<p>
Now, here's the tricky part.  This map j is always a surjection.  And
it will be an <i>isomorphism</i> when the K&auml;hler differentials are a
finitely generated projective module.  But it won't <i>always</i> be
an isomorphism!

<p>
For example, when A is the algebra of rational polynomials on a
simplex, &Omega;<sub>K</sub><sup>1</sup>(A) is a finitely generated
projective module.  In fact it's the free module with one generator
dx<sub>i</sub> for each independent coordinate.  So in this case we
actually get an isomorphism

<p>
&Omega;<sup>1</sup>(A) &cong; &Omega;<sub>K</sub><sup>1</sup>(A)

<p>
and thus 

<p>
&Omega;(A) &cong; &Omega;<sub>K</sub>(A)

<p>
More generally, this is true whenever A is the algebraic functions on
a smooth affine algebraic variety, by the same sort of argument.  So
in these cases, you don't need to worry about the niggling nuances I'm
rubbing your nose in here.

<p>
But when A is the algebra of smooth functions on a manifold, the
1-forms are <em>not</em> the same as the K&auml;hler differentials!

<p>
Indeed, let A be the algebra of smooth functions on the real line.  
Then one can show 

<p>
j: &Omega;<sub>K</sub><sup>1</sup>(A) &rarr; &Omega;<sup>1</sup>(A)

<p>
is not one-to-one.  In fact, David Speyer showed this after Maarten
Bergvelt noticed I was being overoptimistic in assuming otherwise.
You can see Speyer's argument here:

<p>
9) David Speyer, Kahler differentials and ordinary differentials,
Math Overflow, <a href = "http://mathoverflow.net/questions/6074/kahler-differentials-and-ordinary-differentials/9723#9723">http://mathoverflow.net/questions/6074/kahler-differentials-and-ordinary-differentials/9723#9723</a>

<p>
He shows that in &Omega;<sub>K</sub><sup>1</sup>(A), d(e<sup>x</sup>)
is not equal to e<sup>x</sup> dx.  The intuition here is simple:
showing these guys are equal requires actual calculus, with limits and
stuff.  But K&auml;hler differentials are defined purely
algebraically, so they don't know that stuff!

<p>
However, turning this idea into a proof takes work.  It can't be as
easy as I just made it sound!  After all, &Omega;<sup>1</sup>(A) was
<em>also</em> defined purely algebraically, and in here we <em>do</em> have
d(e<sup>x</sup>) = e<sup>x</sup> dx.  Indeed, this is <em>why</em> Speyer's
argument shows that

<p>
j: &Omega;<sub>K</sub><sup>1</sup>(A) &rarr; &Omega;<sup>1</sup>(A)

<p>
fails to be one-to-one.

<p>
So now you should be wondering: how do we know d(e<sup>x</sup>) =
e<sup>x</sup> dx in &Omega<sup>1</sup>(A)?  Since
&Omega;<sup>1</sup>(A) is the dual of the derivations, to show

<p>
d(e<sup>x</sup>) = e<sup>x</sup> dx

<p>
we just need to check that they agree on all derivations.  The hard
part is to prove that any derivation of A is of the form

<p>
v(f) = g f &prime;

<p>
for some g in A, where f &prime; is the usual derivative of f.  Then we have

<p>
d(e<sup>x</sup>)(v) = v(e<sup>x</sup>) = g e<sup>x</sup> =
e<sup>x</sup> v(x) = (e<sup>x</sup> dx)(v)

<p>
so we're done!  

<p>
(Here x is the usual function by that name on the real line - you
know, the one that equals x at the point x.  Sorry - that sounds
really stupid!  But anyway, the derivative of x is 1, so v(x) = g.)

<p>
So here's the hard part.  Say we have a derivation v of the algebra A
of smooth functions on the real line.  Why is there a function g such
that 

<p>
v(f) = g f &prime; 

<p>
for all functions f?  As you can guess from my parenthetical remark,
we should try

<p>
g = v(x)

<p>
So, let's prove

<p>
v(f) = v(x) f &prime; 

<p>
We just need to check they're equal at any point x<sub>0</sub>.  So, let's 
use a kind of Taylor series trick:

<p>
f(x) = f(x<sub>0</sub>) + (x - x<sub>0</sub>) f &prime;(x<sub>0</sub>)
+ (x - x<sub>0</sub>)<sup>2</sup> h(x)

<p>
Here it's utterly crucial that h is a smooth function on the real
line.  Check that yourself!!!  Then, apply the derivation v and use 
the three rules that derivations obey:

<p>
v(f)(x) = v(x) f &prime;(x<sub>0</sub>) + 2(x - x<sub>0</sub>) v(x) h(x) + (x - x<sub>0</sub>)<sup>2</sup> v(h)  

<p>
Then evaluate both sides at x = x<sub>0</sub>.  A bunch of stuff goes away:

<p>
v(f)(x<sub>0</sub>) = v(x) f &prime;(x<sub>0</sub>)

<p>
Since this was true for any point x<sub>0</sub>, we indeed have

<p>
v(f) = v(x) f &prime;

<p>
as desired.  

<p>
Sneaky, huh?  The argument looked &quot;purely algebraic&quot; - but only
because we could pack all the calculus into the utterly crucial
bit that I made you check for yourself.  By the way, this utterly
crucial bit uses the theory of &quot;Hadamard quotients&quot;: if f is 
smooth function on the real line then

<p>
(f(x) - f(y))/(x - y)

<p>
extends to a smooth function on the plane if we define it to be
the derivative of f when x = y.

<p>
A fancier version of this argument works for R<sup>n</sup>.  This in
turn gives the usual proof that that derivations of the algebra A of
smooth functions on a manifold X are the same as smooth vector fields.
And that, in turn, guarantees that &Omega;(A) as defined algebraically
matches the ordinary concept of differential forms on X.  The
K&auml;hler forms are different, but as we've seen, there's a
surjection of DGCAs

<p>
j: &Omega;<sub>K</sub>(A) &rarr; &Omega;(A)

<p>
sending any function f in K&auml;hler land to the same function f in
ordinary differential form land.  

<p>
So that's the story!  It's a bit technical, but if we didn't 
occasionally enjoy being dragged through the mud of technical details,
we wouldn't like math, now, would we?  I think even more details will become 
available here:

<p>
10) nLab, K&auml;hler differential,
<a href = "http://ncatlab.org/nlab/show/K%C3%A4hler+differential">http://ncatlab.org/nlab/show/K%C3%A4hler+differential</a>

<p>
This may be a good place to stop reading, if you don't already love
category theory up to and including &quot;weighted colimits&quot;.  But I can't
resist saying a bit more.  And if you've never understood weighted
colimits, maybe this will make you want to.

<p>
I already told you how we turn any topological space X into a
simplicial set Sing(X) and then back into a bigger topological space
|Sing(X)| and then into a DGCA.

<p>
But if you know homotopy theory well, you know this subject regards
topological spaces and simplicial sets as two different views of
&quot;the same thing&quot;.  So turning a topological space into a
simplicial set is no big deal.  So in fact, the the core of the above
construction is the process that takes a simplicial set and turns it
into a DGCA.  And I'd like to explain this process a bit more
efficiently now.

<p>
Here's the point: this process is a lot like &quot;geometric realization&quot;.
In geometric realization we start with a simplicial set S, which is 
really a functor

<p>
S: &Delta;<sup>op</sup> &rarr; Set

<p>
where &Delta; is the category of simplices.  And we know how to turn
any simplex into a topological space, so we also have a functor

<p>
F: &Delta; &rarr; Top

<p>
We can then take the &quot;weighted colimit&quot; of F with S as our
&quot;weight&quot;.  This creates a topological space |S|, the
&quot;geometric realization&quot; of S.

<p>
The idea is that we take each simplex in our simplicial set, turn
it into a space, and then glue all these spaces together.  For this
trick to work, all we need is that the category Top has colimits.

<p>
Similarly, we know how to turn any simplex into a DGCA, namely the
rational differential forms on that simplex!  So we also have a
functor

<p>
F &prime;: &Delta; &rarr; [DGCAs]<sup>op</sup>

<p>
There's an &quot;op&quot; here because of the usual contravariant relation
between algebra and geometry.  But never mind: what matters is that
DGCA<sup>op</sup> has colimits.  So we can copy what we did before, and take
the weighted colimit of f &prime; with S as our weight.  And now this creates 
a DGCA: the &quot;rational differential forms&quot; on our simplicial set S.

<p>
The idea is that we take each simplex in our simplicial set, turn
it into a DGCA, and then glue all these DGCAs together.  But perhaps
I should say &quot;coglue&quot;, because of that &quot;op&quot;.

<p>
While we're playing these games, I should point out a simpler version.
We also have a functor

<p>
F&quot;: &Delta; &rarr; [commutative algebras]<sup>op</sup>

<p>
and we can pull the same stunt to turn our simplicial set into a 
commutative algebra, which is the algebra of functions that restrict
to polynomials with rational coefficients on each simplex!

<p>
But in this case, there's a super-famous name for the category

<p>
[commutative algebras]<sup>op</sup>

<p>
It's called the category of &quot;affine schemes&quot;.  And so we can think
of this stunt more geometrically as the process of taking an affine
scheme for each simplex and gluing them together to get an affine
scheme for our simplicial set S!   So we're doing a kind of &quot;geometric
realization&quot; with affine schemes replacing topological spacs.  

<p>
This leads up to a question for the experts.  Is there a famous name
for the category

<p>
[DGCAs]<sup>op</sup> ?

<p>
It's <i>related</i> to the category of "simplicial affine schemes",
no?  But it's not quite the same.  Can we think of this category as
consisting of simplicial affine schemes with an extra property?  You
see, this bears heavily on the idea that rational homotopy theory is a
generalization of algebraic geometry, with DGCAs replacing commutative
algebras.

<p>
Finally: the picture at the start of This Week's Finds shows dry ice -
frozen carbon dioxide - on the south pole of Mars:

<p>
9) HiRISE (High Resolution Imaging Science Experiments), 
South polar residual cap monitoring: rare stratigraphic contacts,
<a href = "http://hirise.lpl.arizona.edu/ESP_014379_0925">http://hirise.lpl.arizona.edu/ESP_014379_0925</a>

<p>
This dry ice forms quite a variety of baroque patterns.  I don't
know how it happens!  Here are couple more good pictures:

<p>
10) HiRISE (High Resolution Imaging Science Experiments), 
Evolution of the south polar residual cap,
<a href = "http://hirise.lpl.arizona.edu/PSP_004687_0930">http://hirise.lpl.arizona.edu/PSP_004687_0930</a>

<p>
11) HiRISE (High Resolution Imaging Science Experiments), 
South polar carbon dioxide ice cap,
<a href = "http://hirise.lpl.arizona.edu/ESP_014261_0930">http://hirise.lpl.arizona.edu/ESP_014261_0930</a>

<p>
Patrick Russell wrote a description of the last one:

<p>
<blockquote>

   This HiRISE image is of a portion of Mars' south polar residual ice
   cap. Like Earth, Mars has concentrations of water ice at both poles.

<p>
   Because Mars is so much colder, however, the seasonal ice that gets
   deposited at high latitudes in the winter and is removed in the
   spring (generally analogous to winter-time snow on Earth) is
   actually carbon dioxide ice. Around the south pole there are areas
   of this carbon dioxide ice that do not disappear every spring, but
   rather survive winter after winter. This persistent carbon dioxide
   ice is called the south polar residual cap, and is what we are
   looking at in this HiRISE image.

<p>
   Relatively high-standing smooth material is broken up by
   semi-circular depressions and linear, branching troughs that make a
   pattern resembling those of your fingerprints. The high-standing
   areas are thicknesses of several meters of carbon dioxide ice. The
   depressions and troughs are thought to be caused by the removal of
   carbon dioxide ice by sublimation (the change of a material from
   solid directly to gas). HiRISE is observing this carbon dioxide
   terrain to try to determine how these patterns develop and how fast
   the depressions and troughs grow.

<p>
   While the south polar residual cap as a whole is present every
   year, there are certainly changes taking place within it. With the
   high resolution of HiRISE, we intend to measure the amount of
   expansion of the depressions over multiple Mars years. Knowing the
   amount of carbon dioxide removed can give us an idea of the
   atmospheric, weather, and climate conditions over the course of a
   year.

<p>
   In addition, looking for where carbon dioxide ice might be being
   deposited on top of this terrain may help us understand if there is
   any net loss or accumulation of the carbon-dioxide ice over time,
   which would be a good indicator of whether Mars' climate is in the
   process of changing or not.

</blockquote>

Here's what it looks like:

<p>
<div align = "center">
<img border = "2" src = "014261_0930.jpg">
</div>

<p>
It looks like a white Christmas, just like <a href = 
"http://www.youtube.com/watch?v=sXE0euoKWSk">the one they're having
on the east coast</a> of the United States!  My mom lives in DC,
and I need to call her and find out how she's doing, with all this
snow.

<p>
<HR><p>
<b>Addenda:</b>  I wrote:

<blockquote>

So, let's focus our attention on functions on |Sing(X)| that when
restricted to any simplex give polynomials with rational coefficients.
This is a commutative algebra over the <i>rational</i> numbers.  Call
it A.

</blockquote>

<a href = "http://golem.ph.utexas.edu/category/2009/12/this_weeks_finds_in_mathematic_48.html#c030413">Maarten Bergvelt</a> inquired:

<blockquote>

Is it obvious that there are any such functions?

</blockquote>

And this was a good question.  The constant functions obviously work,
but we'd really like at least enough functions of this sort to
&quot;separate points&quot; on |Sing(X)|.  We say a collection of
functions on a space &quot;separate points&quot; if for any two points
x &ne; y in that space, we can find a function f in our collection
with f(x) &ne; f(y).

<p>
And indeed, we'd like this to work for any simplicial set.  Given a simplicial
set S, we can define an algebra A of real-valued functions on |S| that
are rational polynomials when restricted to each simplex.  Do the functions
in A separate points of |S|?

<p>
Over at the n-Category Caf&eacute; we showed the answer is <i>yes</i>.
The key lemma is this:

<blockquote>

   <b>Conjecture:</b> Suppose we are given an n-simplex and a continuous
   function f on its boundary which is a rational polynomial on each
   face.  Then f extends to a rational polynomial on the whole
   n-simplex.

</blockquote>

<a href = "http://golem.ph.utexas.edu/category/2009/12/this_weeks_finds_in_mathematic_48.html#c030432">David Speyer</a> explained how to prove it.

<P>
<a href =
"http://golem.ph.utexas.edu/category/2009/12/this_weeks_finds_in_mathematic_48.html#c030472">Maarten
Bergvelt</a> also caught a big mistake.  I had thought the smooth
1-forms on smooth manifold were the same as the K&auml;hler
differentials for its algebra of smooth functions.  Maarten doubted
this - and <a href =
"http://golem.ph.utexas.edu/category/2009/12/this_weeks_finds_in_mathematic_48.html#c030518">David
Speyer</a> was able to prove it's wrong!  (His proof uses the axiom of
choice, since it involves a nonprincipal ultrafilter.  Do we
<i>need</i> the axiom of choice here?)

<P>
This led to a big discussion, which I've attempted to summarize in the
above improved version of &quot;<a href =
"week287.html">week287</a>&quot;.  To see the discussion we had, and
add your comments, visit <a href =
"http://golem.ph.utexas.edu/category/2009/12/this_weeks_finds_in_mathematic_48.html"><em>n</em>-Category
Caf&eacute;</a>.


<p>
<HR><p>
<em>We live on an island surrounded by a sea of ignorance.  As our island
of knowledge grows, so does the shore of our ignorance.</em> - John Wheeler

<P>
<HR><P>
<!-- BEGIN FOOTER -->
&#169; 2009  John Baez<br>
baez@math.removethis.ucr.andthis.edu <br>
<P>
<TABLE WIDTH = 100%> <TR>
<TD WIDTH=10%>
<A HREF = "week286.html">
   <img border = none; src="lastweek.png"></A>
<TD WIDTH=80%>
<CENTER>
<A HREF="README.html">
  <img border = none; src="home.png"><br>
</A>
<A HREF="http://math.ucr.edu/home/baez/TWF.html">
    <img border = none; src="contents.png">
</A>
</CENTER>
<TD WIDTH=10%>
<A HREF = "week288.html">
  <img border = none; src="nextweek.png">
</A>
</TABLE><!-- END FOOTER -->
