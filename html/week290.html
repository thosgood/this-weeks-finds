<TITLE> week290 </TITLE>

<!-- BEGIN HEADER -->
<TABLE WIDTH = 100%> <TR>
<TD WIDTH=10%>
<A HREF = "week289.html">
   <img border = none; src="lastweek.png"></A>
<TD WIDTH=80%>
<CENTER>
<A HREF="README.html">
  <img border = none; src="home.png"><br>
</A>
<A HREF="http://math.ucr.edu/home/baez/TWF.html">
    <img border = none; src="contents.png">
</A>
</CENTER>
<TD WIDTH=10%>
<A HREF = "week291.html">
  <img border = none; src="nextweek.png">
</A>
</TABLE>
<H4> January 15, 2010 </H4>
<H2> This Week's Finds in Mathematical Physics (Week 290) </H2>
<H4> John Baez </H4>
<!-- END HEADER -->


<P>
This week we'll start with a math puzzle, then a paper about
categorification in analysis.  Then we'll continue learning about
electrical circuits and their analogues in other branches of physics.
We'll wrap up with a bit more rational homotopy theory.  

<P>
But first: here's an image that's been making the rounds lately.
What's going on here?

<P>
<div align = "center">
<img border = "2" src = "http://math.ucr.edu/home/baez/007962_2635.jpg">
</div>

<P>
Next: a math puzzle!  This was created by a correspondent who wishes
to remain anonymous.   Here are some numbers.  Each one is the number of
elements in some famous mathematical gadget.  What are these numbers -
and more importantly, what are these gadgets?

<P>
<ul>
<li>
How many minutes are in an hour? 
</li>
<li>
How many hours are in a week?  
</li>
<li>
How many hours are in 3 weeks? 
</li>
<li>
How many feet are in 1.5 miles? 
</li>
<li>
How many minutes are in 2 weeks? 
</li>
<li>
How many inches are in 1.5 miles?
</li>
<li>
How many seconds are in a week?  
</li>
<li>
How many seconds are in 3 weeks? 
</li>
</ul>

<P>
The answers are at the end. 

<P>
The wave of categorification overtaking mathematics is finally hitting
analysis!  I spoke a tiny bit about this in &quot;<a href =
"week274.html">week274</A>&quot;, right after I'd finished a paper
with Baratin, Freidel and Wise on infinite-dimensional representations
of 2-groups.  I thought it would take a long time for more people to
get interested in the blend of 2-categories and measure theory that we
were exploring.  After all, there's a common stereotype that says
mathematicians who like categories hate analysis, and vice versa.
But I was wrong:

<P>
1) Goncalo Rodrigues, Categorifying measure theory: a roadmap,
available as <a href = "http://arxiv.org/abs/0912.4914">arXiv:0912.4914</a>.

<P>
Read both papers together and you'll get a sense of how much there is
to do in this area!  A lot of basic definitions remain up for grabs.
For example, Rodrigues' paper defines &quot;2-Banach spaces&quot;, but
will his definition catch on?  It's too soon to tell.  There are
already lots of theorems.  And there's no shortage of interesting
examples and applications to guide us.  But finding the best framework
will take a while.  I urge anyone who likes analysis and category
theory to jump into this game while it's still fresh.

<P>
But my own work is taking me towards mathematics of a more
applied sort.  My excuse is that I'll be spending a year in Singapore
at the Centre for Quantum Technologies, starting in July.  This will
give me a chance to think about computation, and condensed matter
physics, and quantum information processing, and diagrams for physical
systems built from pieces.  Such systems range from the humble
electrical circuits that I built as a kid, to integrated circuits, to
fancy quantum versions of these things.

<P>
So, lately I've been talking about a set of analogies relating various
types of physical systems.  I listed 6 cases where the analogies
are quite precise:

<PRE>

                displacement    flow          momentum      effort
                     q          dq/dt            p          dp/dt

Mechanics       position       velocity       momentum      force
(translation)

Mechanics       angle          angular        angular       torque
(rotation)                     velocity       momentum

Electronics     charge         current        flux          voltage
                                              linkage

Hydraulics      volume         flow           pressure      pressure
                                              momentum

Thermodynamics  entropy        entropy        temperature   temperature
                               flow           momentum

Chemistry       moles          molar          chemical      chemical
                               flow           momentum      potential

</PRE>
This week I'd like to talk about five circuit elements that we can use
to build more complicated electrical circuits: resistors, inductors,
capacitors, voltage sources, and current sources.  I'll tell you the
basic equations they obey, and say a bit about their analogues in the
mechanics of systems with translational degrees of freedom.  They also
have analogues in the other rows.

<P>
Engineers call these five circuit elements &quot;1-ports&quot;.  A
1-port can be visualized as a black box with 2 places where you can
stick in a wire:

<PRE>
        |
        |
      -----
     |     |
     |     |
      -----
        |
        |
</PRE>

More generally, an &quot;n-port&quot; has 2n places where you can
attach a wire.  This numbering system may seem peculiar.  Indeed, it
overlooks circuits that have an odd number of wires coming out, like
this one made of just wires:

<PRE>
     \     /
      \   /
       \ /
        |
        |
        |
</PRE>

You can use gizmos like this to stick together 1-ports 
&quot;in parallel&quot;:


<PRE>
        |
        |
        |
       / \
      /   \
     /     \
   ---     ---
  |   |   |   |
   ---     ---
     \     /
      \   /
       \ /
        |
        |
        |
</PRE>

However, if you've ever looked at the back of a TV or stereo, you'll
see that place where you can plug in cables tend to come in pairs!
Each pair is called a &quot;port&quot;.  So, electrical engineers
often - though not always - focus on n-ports, where the wires coming
out are grouped in pairs.  And there's probably even a good
<i>mathematical</i> reason for paying special attention to these -
something related to symplectic geometry.  That's one of the things
I'm trying to understand better.

<P>
Later I'll tell you about some famous 2-ports and 3-ports, but today
let's do 1-ports.  If we have a 1-port with wires coming out of it, we
can arbitrarily choose one wire and call it the the &quot;input&quot;,
with the other being the &quot;output&quot;:

<PRE>
        |
        V
        |
      -----
     |     |
     |     |
      -----
        |
        V
        |
</PRE>
If you know a little category theory, this should seem suspiciously
similar to a &quot;morphism&quot;.  And if you know a bit more, this
should remind you of other situations where it takes an arbitrary
choice to distinguish between the &quot;input&quot; and the
&quot;output&quot; of a morphism.

<P>
Any 1-port has a &quot;flow through it&quot; and an &quot;effort
across it&quot;, which which are functions of time.  Remember,
&quot;flow&quot; is the general concept that reduces to current in the
special case of electronics.  &quot;Effort&quot; is the concept that
reduces to voltage.

<P>
The time integral of flow is called the &quot;displacement&quot; and
denoted q, and the time integral of effort is called the
&quot;momentum&quot; and denoted p.  So, flow is q' = dq/dt and effort
is p' = dp/dt.

<P>
To mathematically specify a 1-port, we give one equation involving p,
q, p', q', and the time variable t.  Here's how it works for the five
most popular types of 1-ports:

<ol>
<li> 
  A &quot;resistance&quot;.  This is the general term for what we
  call a &quot;resistor&quot; in the case of electrical circuits, and
  &quot;friction&quot; in mechanics.  In hydraulics, you can make
  a resistance using a narrowed pipe:

<P>
<div align = "center">
<img src = "electronics_analogy_reduced_pipe_resistor.png">
</div>
<P>

In all cases, the effort is some function of the flow:

<P>
  p' = f(q')

<P>
  An easy special case is a linear resistance, for which the effort
  is proportional to the flow:

<P>
  p' = R q'

<P>
  Here R is some constant, also called the &quot;resistance&quot;.  In
  electric circuit theory this equation is called Ohm's law, and
  people write it using different symbols.  Note we need to be careful
  about our sign conventions: in mechanics we usually think of
  friction as giving force = R velocity with R <i>negative</i>, while
  in electric circuit theory we usually think of an ordinary resistor
  as giving voltage = R current with R <i>positive</i>.  The two cases
  are not fundamentally different: it's just an artifact of differing
  sign conventions!

<P>
</li>
<li> A &quot;capacitance&quot;.  This is the general term for what we
  call a &quot;capacitor&quot; in the case of electrical circuits, or
  a &quot;spring&quot; in mechanics.  In hydraulics, you can make a
  capacitance out of a tank with pipes coming in from both ends and a
  rubber sheet dividing it in two:

<P>
<div align = "center">
<img src = "electronics_analogy_flexible_tank_capacitor.png">
</div>
<P>
  In all cases, the displacement is some
  function of effort:

<P>
  q = f(p')

<P>
  An easy special case is a linear capacitance, for which the 
  displacement is proportional to the effort:

<P>
  q = C p'

<P>
  Here C is some constant, also called the &quot;capacitance&quot;.
  Again we need to be careful with our conventions: in mechanics we
  usually think of a spring as being stretched by an amount equal to
  1/k times the force applied.  Here k, the <i>reciprocal</i> of C, is
  called the spring constant.  But some engineers work with C and call
  it the &quot;compliance&quot; of the spring.  An easily stretched
  spring has big C, small k.

<P>
</li>
<li> 
  An &quot;inertance&quot;.  This is the general term for what we
  call an &quot;inductor&quot; in the case of electrical circuits, or
  a &quot;mass&quot; in mechanics.  The weird word
  &quot;inertance&quot; hints at how mass gives a particle inertia.
  In hydraulics, you can build an inertance by putting a heavy turbine
  inside a pipe: this makes the water want to keep flowing at the same
  rate.

<P>
  In all cases, the momentum is some function of flow:

<P>
  p = f(q')

<P>
  An easy special case is a linear inertance, for which the 
  momentum is proportional to the flow:

<P>
  p = L q'

<P>
  Here L is some constant, also called the &quot;inertance&quot;.  In
  the case of mechanics, this would be the mass.

<P>
</li>
<li>
  An &quot;effort source&quot;.  This is the general term for what we
  call a &quot;voltage source&quot; in the case of electrical
  circuits, or an &quot;external force&quot; in mechanics.  In 
  hydraulics, an effort source is a compressor set up to maintain a 
  specified pressure difference between the input and output:

<P>
<div align = "center">
<img width = "250" src = "electronics_analogy_compressor_effort_source.gif">
</div>
<P>
Here the
equation is of different type than before!  It can involve the time
variable t:

<P>
  p' = f(t)
  
<P>
</li>
<li>
  A &quot;flow source&quot;.  This is the general term for what we
  call a &quot;current source&quot; in the case of electrical
  circuits.  In hydraulics, an flow source is a pump set up to
  maintain a specified flow.

<P>
Here the equation is

<P>
  q' = f(t)
</li>
</ol>

It's interesting to ponder these five 1-ports and how they form
families.

<P>
The voltage and current sources form a family, since only these
involve the variable t in an explicit way.  Also, only these can be
used to add energy to a circuit.  So, these two are called
&quot;active&quot; circuit elements.

<P>
The other three are called &quot;passive&quot;.  Among these, the
capacitance and inertance form a family because they both conserve
energy.  The resistance is different: it dissipates energy - or more
precisely, turns it into heat energy, which is not part of our simple
model.  If you're more used to mechanics than electrical circuits, let
me translate what I'm saying into the language of mechanics: a machine
made out of masses and springs will conserve energy, but friction
dissipates energy.

<P>
Let's try to make this &quot;energy conservation&quot; idea a bit more
precise.  I've already said that p'q', effort times flow, has
dimensions of power - that is, energy per time.  Indeed, for any
1-port, the physical meaning of p'q' is the rate at which energy is
being put in.  So, in electrical circuit theory, people sometimes say
energy is &quot;conserved&quot; if we can find some function H(p,q)
with the property that

<P>
 dH(p,q)/dt = p'q'

<P>
This function H, called the &quot;Hamiltonian&quot;, describes the
energy stored in the 1-port.  And this equation says that the energy
stored in the system changes at a rate equal to the rate at which
energy is put in!  So energy doesn't get lost, or appear out of
nowhere.

<P>
Now, when I said &quot;energy conservation&quot;, you may have been
expecting something like dH/dt = 0.  But we only get that kind of
energy conservation for &quot;closed&quot; systems - systems that
aren't interacting with the outside world.  We'll indeed get dH/dt = 0
when we build a big circuit with no inputs and no outputs out of
circuit elements that conserve energy in the above sense.  The energy
of the overall system will be conserved, but of course it can flow in
and out of the various parts.

<P>
But of course it's really important to think about circuits with
inputs and outputs - the kind of gizmo you actually plug into the
wall, or hook up to other gizmos!  So we need to generalize classical
mechanics to &quot;open&quot; systems: systems that can interact with their
environment.  This will let us study how big systems are made of
parts.  

<P>
But right now we're just studying the building blocks - and only the
simplest ones, the 1-ports.

<P>
Let's see how energy conservation works for all five 1-ports.  For
simplicity I'll only do the linear 1-ports when those are available,
but the results generalize to the nonlinear case:

<P>
<ol>
<li>
   The &quot;resistance&quot;.   For a linear resistance we have

<P>
  p' = R q'

<P>
  so the power is

<P>
  p'q' = R (q')<sup>2</sup>

<P>
  In the physically realistic case R &gt; 0 so this is nonnegative,
  meaning that we can only put energy <i>into</i> the resistor.
  And note that p'q' is <i>not</i> the time derivative of some 
  function of p and q, so energy is not conserved.  We say the
  resistance &quot;dissipates&quot; energy.

<P>
</li>
<li>
   The &quot;capacitance&quot;.  For a linear capacitance we have

<P>
  q = C p'

<P>
  so the power is 

<P>
  p'q' = qq' / C

<P>
  Note that unlike the resistor this can take either sign, even
  in the physically realistic case C &gt; 0.  More importantly, in
  this case p'q' is the time derivative of a function of p and q,
  namely 

<P>
  H(p,q) = q<sup>2</sup> / 2C

<P>
  So in this case energy is conserved.  If you're comfortable
  with mechanics you'll remember that a spring is an example of
  a capacitance, and H(p,q) is the usual &quot;potential energy&quot; of a 
  spring when C is the reciprocal of the spring constant.
  
<P>
</li>
<li>
   The &quot;inertance&quot;.  For a linear inertance

<P>
  p = L q'

<P>
  so the power is

<P>
  p'q' = pp' / L

<P>
  Again this can take either sign, even in the physically realistic
  case L &gt; 0.  And again, p'q' is the time derivative of a function
  of p and q, namely

<P>
  H(p,q) = p<sup>2</sup> / 2L

<P>
  So energy is also conserved in this case.  If you're comfortable
  with mechanics you'll remember that a mass is an example of
  a inertance, and H(p,q) is the usual &quot;kinetic energy&quot; of a mass
  when L equals the mass.

<P>
</li>
<li>
   The &quot;effort source&quot;.  For an effort source

<P>
  p' = f(t)

<P>
  for some function f, so the power is

<P>
  p'q' = f(t) q'

<P>
  This is typically not the time derivative of some function of
  p and q, so energy is not usually conserved.  I leave it
  as a puzzle to give the correct explanation of what's going on
  when f(t) is a constant.  

<P>
</li>
<li>
  The &quot;flow source&quot;.  For a flow source

<P>
  q' = f(t)

<P>
  for some function f, so the power is

<P>
  p'q' = f(t) p'

<P>
  This is typically not the time derivative of some function of
  p and q, so energy is not usually conserved.  Again, I
  leave it as a puzzle to understand what's going on when f(t)
  is constant.
</li>
</ol>

<P>
So, everything works as promised.  But if your background in classical
mechanics is anything like mine, you should still be puzzled by the
equation

<P>
dH(p,q)/dt = p'q'

<P>
This is sometimes called the &quot;power balance equation&quot;.  But you mainly
see it in books on electrical engineering, not classical mechanics.
And I think there's a reason.  I don't see how to derive it from a
general formalism for classical mechanics, the way I can derive dH/dt
= 0 in Hamiltonian mechanics.  At least, I don't see how when we write
the equation this way.  I think we need to write it a bit differently!

<P>
In fact, I was quite confused until Tim van Beek pointed me to a 
nice discussion of this issue here:

<P>
2) Bernard Brogliato, Rogelio Lozano, Bernhard Maschke and Olav
Egeland, Dissipative Systems Analysis and Control: Theory and
Applications, 2nd edition, Springer, Berlin, 2007.

<P>
I'll say more about this later.  For now let me just explain two
buzzwords here: &quot;control theory&quot; and &quot;dissipative systems&quot;.

<P>
Traditional physics books focus on closed systems.  &quot;Control theory&quot;
is the branch of physics that focuses on open systems - and how to
make them do what you want!  

<P>
For example, suppose you want to balance a pole on your finger.  How
should you move your finger to keep the pole from falling over?
That's a control theory problem.  You probably don't need to read a
book to solve this particular problem: we're pretty good at learning
to do tricks like this without thinking about math.  But if you wanted
to build a robot that could do this - or do just about anything -
control theory might help.

<P>
What about &quot;dissipative systems&quot;?  I already gave an
example: a circuit containing a resistor.  I talked about another in
&quot;<a href = "week288.html">week288</A>&quot;: a mass on a spring
with friction.  In general, a dissipative system is one that loses
energy, or more precisely converts it to heat.  We often don't want to
model the molecular wiggling that describes heat.  If we leave this
out, dissipative systems are not covered by ordinary Hamiltonian
mechanics - since that framework has energy conservation built in.
But there are generalizations of Hamiltonian mechanics that include
dissipation!  And these are pretty important in practical subjects
like control theory... since life is full of friction, as you've
probably noticed.

<P>
So, this book covers everything that <i>my</i> classical mechanics
education downplayed or left out: open systems, dissipation and
control theory!  And in the chapter on &quot;dissipative physical
systems&quot;, it derives power balance equations for
&quot;input-output Hamiltonian systems&quot; and &quot;port-controlled
Hamiltonian systems&quot;.  Apparently it's the latter that describes
physical systems built from n-ports.  

<P>
For more on port-controlled Hamiltonian systems, this book recommends:

<P>
3) B. M. Maschke and A. J. van der Schaft, Port controlled Hamiltonian
systems: modeling origins and system theoretic properties, in
Proceedings of the 2nd IFAC Symp. on Nonlinear Control Systems Design,
NOLCOS'92 (1992), pp. 282-288,

<P>
4) B. M. Maschke and A. J. van der Schaft, The Hamiltonian formulation
of energy conserving physical systems with ports, Archiv fur
Elektronik und Ubertragungstechnik 49 (1995), 362-371.

<P>
5) A. J. van der Schaft, L<sup>2</sup>-gain and Passivity Techniques in
Nonlinear Control, 2nd edition, Springer, Berlin, 2000.

<P>
So, I need I learn more about this stuff, and then explain it to you.
But let's stop here for now, and turn to... rational homotopy theory!

<P>
Nothing big this week: I just want to take stock of where we are.
I've been trying to explain a triangle of concepts:

<PRE>
                      RATIONAL SPACES
                         /      \  
                        /        \  
                       /          \  
                      /            \
                     /              \
      DIFFERENTIAL GRADED ------- DIFFERENTIAL GRADED
      COMMUTATIVE ALGEBRAS           LIE ALGEBRAS
</PRE>

<P>
In &quot;<a href = "week287.html">week287</A>&quot; I explained a
functor going down the left side of this triangle.  In fact I
explained how we can get a differential graded commutative algebra, or
DGCA, from <i>any</i> topological space.  This involved a grand
generalization of differential forms.

<P>
In &quot;<a href = "week289.html">week289</A>&quot; I explained a
functor going down the right side.  In fact I explained how we can get
a differential graded Lie algebra, or DGLA, from <i>any</i>
topological space with a chosen basepoint.  This involved a grand
generalization of Lie groups, and their Lie algebras.

<P>
Today I'd like to explain a sense in which all three concepts in this
triangle are &quot;the same&quot;.  I won't give you the best possible
theorem along these lines - just Quillen's original result, which is
pretty easy to understand.  It says that three categories are
equivalent: one for each corner of our triangle!

<P>
I explained the first category back in &quot;<a href =
"week286.html">week286</a>&quot;.  I called it the &quot;rational
homotopy category&quot;, and I described it in several ways.  Here's
one.  Start with the category where:

<ul>
<li>the
objects are 1-connected pointed spaces;
</li>
<li>
the morphisms are basepoint-preserving maps.
</li>
</ul>

Then, throw in formal inverses to all &quot;rational homotopy
equivalences&quot; - that is, maps

<P>
f: X &rarr; X'

<P>
that give isomorphisms between rational homotopy groups:

<P>
Q &otimes; &pi;<sub>n</sub>(f): Q &otimes; &pi;<sub>n</sub>(X) 
&rarr; Q &otimes; &pi;<sub>n</sub>(X')

<P>
This gives the rational homotopy category.

<P>
The second category involves DGCAs.  Well - actually not.  To get the
nicest results, it seems we should work dually and use differential
graded <i>co</i>commutative <i>co</i>algebras, or DGCCs.  I'm sorry to
switch gears on you like this, but that's life.  The difference is
&quot;purely technical&quot;, but I want to state a theorem that I'm
sure is true!

<P>
In &quot;<a href = "week287.html">week287</a>&quot; we saw how
Sullivan took any space and built a DGCA whose cohomology was the
rational cohomology of that space.  But today let's follow
Quillen and instead work with a DGCC whose homology is the rational
homology of our space.  

<P>
So, let's start with the category of DGCC's over the rational
numbers - but not <i>all</i> of them, only those that are trivial in
the bottom two dimensions:

<PRE>
    d      d       d       d
0 &lt;--- 0 &lt;--- C<sub>2</sub> &lt;--- C<sub>3</sub> &lt;--- ...
</PRE>

Why?  Because our spaces are 1-connected, so their bottom two
homology groups are boring.  Then, let's throw in formal inverses to
&quot;quasi-isomorphisms&quot; - that is, maps between DGCCs

<P>
f: C &rarr; C'

<P>
that give isomorphisms between homology groups:

<P>
H<sub>n</sub>(f): H<sub>n</sub>(C) &rarr; H<sub>n</sub>(C')

<P>
The resulting category is <i>equivalent</i> to the rational 
homotopy category!

<P>
The third category involves DGLAs.  We start with the category of
DGLAs over the rational numbers - but not <i>all</i> of them, only
those that are trivial in the bottom dimension:

<PRE>
    d       d       d      d
0 &lt;--- L<sub>1</sub> &lt;--- L<sup>2</sup> &lt;--- L<sup>3</sup> &lt;--- ...
</PRE>

Just the very bottom dimension, not the bottom two!  Why?  Because we
get a DGLA from the group of <i>loops</i> in our rational space, and
looping pushes down dimensions by one.  Then, we throw in formal
inverses to &quot;quasi-isomorphisms&quot; - that is, maps between
DGLAs:

<P>
f: L &rarr; L'

<P>
that give isomorphisms between homology groups:

<P>
H<sub>n</sub>(f): H<sub>n</sub>(L) &rarr; H<sub>n</sub>(L')

<P>
Again, the resulting category is <i>equivalent</i> to the rational 
homotopy category!

<P>
So, we have a nice unified picture.  We could certainly improve it in
various ways.  For example, I haven't discussed the bottom edge of the
triangle.  Doing this quickly brings in L<sub>&infin;</sub>-algebras,
which are like DGLAs where all the laws hold only &quot;up to chain
homotopy&quot;.  It also brings in gadgets that are like DGCAs or
DGCCs, but where all the laws hold only up to chain homotopy.  This
outlook eventually leads us to realize that we have something much
better than three equivalent categories.  We have three equivalent
(&infin;,1)-categories!

<P>
But there's also the question of what we can <i>do</i> with this
triangle of concepts.  There are lots of classic applications
to topology, and lots of new applications to mathematical
physics.  

<P>
So, there's more to come.

<P>
As for the number puzzle at the beginning, all the numbers I listed
are the sizes of various &quot;finite simple groups&quot;.  These
are the building blocks from which all finite groups can be built.
You can see a list of them here:

<P>
6) Wikipedia, Finite simple groups, 
<a href = "http://en.wikipedia.org/wiki/List_of_finite_simple_groups">http://en.wikipedia.org/wiki/List_of_finite_simple_groups</a>

<P>
There are 16 infinite families and 26 exceptions, called "sporadic"
finite simple groups.  Anyway, here we go:

<P>
<ul>
<li>
How many minutes are in an hour? 
<P>
60, which is the number of elements in the smallest nonabelian finite
simple group, namely A<sub>5</sub>.  Here A<sub>n</sub> is an an
&quot;<a href =
"http://en.wikipedia.org/wiki/Alternating_group">alternating
group</a>&quot;: the group of even permutations of the set with n
elements.  By some wonderful freak of nature, A<sub>5</sub> is
isomorphic to both PSL(2,4) and PSL(2,5).  Here PSL(n,q) is a &quot;<a
href =
"http://en.wikipedia.org/wiki/Projective_linear_group">projective
special linear group</a>&quot;: the group of determinant-1 linear
transformations of an n-dimensional vector space over the field with q
elements, modulo its center.
<P>
</li>
<li>
How many hours are in a week?  

<P>
168, which is the number of elements - or &quot;order&quot; - of the second
smallest nonabelian finite simple group, namely PSL(2,7).  Thanks to
another marvelous coincidence, this is isomorphic to PSL(3,2).  See
&quot;<a href = "week214.html">week214</A>&quot; for a lot more about this group and its relation to Klein's
quartic curve and the Fano plane.

<P>
</li>
<li>
How many hours are in 3 weeks?  

<P>
504, which is the order of the finite simple group PSL(2,8).

<P>
</li>
<li>
How many feet are in 1.5 miles?  

<P>
7,920, which is the order of the finite simple group M<sub>11</sub> - the
smallest of the finite simple groups called <a href =
"http://en.wikipedia.org/wiki/Mathieu_group"> Mathieu groups</a>.  See
&quot;<a href = "week234.html">week234</A>&quot; for more about this.

</li>
<li>
<P>
How many minutes are in 2 weeks?   

<P>
20,160, which is the order of the finite simple group A<sub>8</sub>.
Thanks to another marvelous coincidence, this is isomorphic to
PSL(4,2).  And there's also another nonisomorphic finite simple group
of the same size, namely PSL(3,4)!

</li>
<li>
<P>
How many inches are in 1.5 miles?  

<P>
95,040, which is the order of the finite simple group M<sub>12</sub> -
the second smallest of the Mathieu groups.  See
&quot;<a href = "week234.html">week234</A>&quot; for more about this
one, too.

</li>
<li>
<P>
How many seconds are in a week?  

<P>
604,800, which is the order of the finite simple group J<sub>2</sub> -
the <a href = "http://en.wikipedia.org/wiki/Hall%E2%80%93Janko_group">second 
Janko group</a>, also called the Hall-Janko group.  I don't know anything about
the Janko groups.  They don't seem to have much in common except being
sporadic finite simple groups that were discovered by Janko. 

<P>
I like
what the Wikipedia says about the <a href = "http://en.wikipedia.org/wiki/Janko_group_J3">third Janko group</a>: it &quot;seems unrelated
to any other sporadic groups (or to anything else)&quot;.  Unrelated
to anything else?  Zounds!

</li>
<li>
<P>
How many seconds are in 3 weeks?  

<P>
1,814,400, which is the order of the finite simple group A<sub>10</sub>.
</li>
</ul>

If you like this sort of stuff, you might enjoy this essay:

<P>
7) John Baez, Why there are 63360 inches in a mile?, <a href =
"http://math.ucr.edu/home/baez/inches.html">http://math.ucr.edu/home/baez/inches.html</a>

<P>
It's a curious number:

<P>
63360 = 2<sup>7</sup> &times; 3<sup>2</sup> &times; 5 &times; 11

<P>
It seems rather odd that this number is divisible by 11.
Find out why it is!  

<P>
Finally, what about that image?  Unsurprisingly, it's from Mars.  It
shows a dune field less than 400 kilometers from the north pole,
bordered on both sides by flat regions - but also a big cliff on one
side:

<P>
<div align = "center">
<a href = "http://hirise-pds.lpl.arizona.edu/PDS/EXTRAS/RDR/PSP/ORB_007900_007999/PSP_007962_2635/PSP_007962_2635_RED.abrowse.jpg">
<img border = "2" width = "500" src = "mars_dunes_with_cliff.jpg">
</a>
</div>

<P>
8) HiRISE (High Resolution Imaging Science Experiments), 
Falling material kicks up cloud of dust on dunes,
<a href = "http://hirise.lpl.arizona.edu/PSP_007962_2635">http://hirise.lpl.arizona.edu/PSP_007962_2635</a>

<P>
Some streaks on the dunes look like stands of trees lined up on hilltops: 

<P>
<div align = "center">
<a href = "http://hirise.lpl.arizona.edu/images/2009/details/cut/PSP_007962_2635_cut.jpg">
<img border = "2" src = "mars_dunes.jpg">
</a>
</div>
<P>

It would be great if there were trees on Mars, but it's not true.  In
fact what you're seeing are steep slopes with dark stuff slowly
sliding down them!  Here's a description written by Candy Hansen, a
member of NASA's Mars Reconnaissance Orbiter team at the University of
Arizona:

<blockquote>

There is a vast region of sand dunes at high northern latitudes on
Mars. In the winter, a layer of carbon dioxide ice covers the dunes,
and in the spring as the sun warms the ice it evaporates. This is a
very active process, and sand dislodged from the crests of the dunes
cascades down, forming dark streaks.

<P>
<div align = "center">
<img border = "2" src = "mars_dunes_subimage.jpg">
</div>
<P>

In the subimage falling material has kicked up a small cloud of
dust. The color of the ice surrounding adjacent streaks of material
suggests that dust has settled on the ice at the bottom after similar
events.

<P>
Also discernible in this subimage are polygonal cracks in the ice on
the dunes (the cracks disappear when the ice is gone).

</blockquote>

<p>
<hr>
<p>
<b>Addenda:</b> I thank Toby Bartels, Bruce Smith, and Don Davis of
Boston for some corrections.

<p>
In particular, the number of inches in a mile is divisible by 11
because there are 33/2 feet in a rod.  For the explanation of
<i>that</i>, see my <a href = "inches.html">webpage</a>.  But Don
Davis pointed out that this is not the only reason why the number 11
appears in the American system of units.  A US liquid gallon is 231 =
3 &times; 7 &times 11 cubic inches!

<P>
Why?  According to Don Davis and the Wikipedia article on <a href =
"http://en.wikipedia.org/wiki/Gallon#Historyhttp://en.wikipedia.org/wiki/Gallon#History">gallons</a>)
the reason is that once upon a time, a British wine gallon was 7
inches across and 6 inches deep - for some untold reason that deserves
further investigation.  If we approximate &pi; by 22/7, the volume
then comes out to 3 &times; 7 &times 11 cubic inches!

<P>
This 11-ness of the gallon then infects other units of volume.
For example, a US liquid ounce is 

<P>
3 &times; 7 &times; 11 / 2<sup>7</sup>
<P>

cubic inches!  

<p>
My friend Bruce Smith says that his young son Peter offered the
following correction to the quote of the week: it should really be
<i>&quot;The most important thing is to keep the 2nd most important
thing the 2nd most important thing&quot;</i> - because the first most
important thing is the topic of the sentence!

<P>
John McKay writes:

<blockquote>

   You say you don't know anything about the Janko groups. Let me 
   help you...

<P>
   The first Janko group is a subgroup of G<sub>2</sub>(11). It is
   called J<sub>1</sub> and has order = 11 &times; (11+1) &times;
   (11<sup>3</sup>-1) suggesting incorrectly it may be one of a
   family.  This is the first of the modern sporadics. Then came
   J<sub>2</sub> and J<sub>3</sub> both having isomorphic involution
   centralizers. The first was constructed by Marshall Hall and the
   second by Graham Higman and me.

<P>
   David Wales and I decided on the names so that J<sub>k</sub> has a
   Schur multiplier (=second cohomology group) of order k.
   J<sub>2</sub> is the Hall-Janko group.  Janko finally produced his
   fourth group J<sub>4</sub> (which unfortunately does not have a
   Schur multiplier of order 4)!  J<sub>1</sub>,J<sub>3</sub>, and
   J<sub>4</sub> are among the Pariahs (as are O'Nan, Rud, Ly-Sims).
   They are those sporadics that have no involvement with M = the
   Monster group (see Mark Ronan's book).

<P>
   This group, M, appears to have incredible connections with many
   areas of mathematics and of physics. Its real nature has yet to be
   revealed.

<P>
   Best,<br/>
   John
</blockquote>

<P>
Here G<sub>2</sub>(11) is like the exceptional Lie group G<sub>2</sub>
except it's defined over the field with 11 elements.  So, the number
11 raises its ugly head yet again!

<p>
For more discussion, visit the <a href = "http://golem.ph.utexas.edu/category/2010/01/this_weeks_finds_in_mathematic_51.html"><i>n</i>-Category Caf&eacute;</a>.

<p>
<hr>
<p>
<em>The most important thing is to keep the most important thing the most
important thing.</em> - Donald P. Coduto

<p>
<HR><P>
<!-- BEGIN FOOTER -->
&#169; 2010  John Baez<br>
baez@math.removethis.ucr.andthis.edu <br>
<P>
<TABLE WIDTH = 100%> <TR>
<TD WIDTH=10%>
<A HREF = "week289.html">
   <img border = none; src="lastweek.png"></A>
<TD WIDTH=80%>
<CENTER>
<A HREF="README.html">
  <img border = none; src="home.png"><br>
</A>
<A HREF="http://math.ucr.edu/home/baez/TWF.html">
    <img border = none; src="contents.png">
</A>
</CENTER>
<TD WIDTH=10%>
<A HREF = "week291.html">
  <img border = none; src="nextweek.png">
</A>
</TABLE><!-- END FOOTER -->
