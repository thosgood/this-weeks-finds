<TITLE> week295 </TITLE>




<P>
This week I'll talk about the principle of least power, and Poincar&eacute;
duality for electrical circuits, and a generalization of Hamiltonian
mechanics that people have introduced for dissipative systems.  But first....

<p>
<div align = "center">
<img border = "2" src = "Eyjafjallojokull_brynjar_gaudi.jpg">
<br/>
<font size = "-1">
photo by Brynjar Gaudi / AP
</font>
</a>
</div>

<p>
Now and then the world does something that forcefully reminds us
of its power.  As you probably know, the Eyjafjallajökull
volcano in Iceland is emitting a plume of glass dust which has brought
air traffic to a halt over much of Europe.  This dust is formed 
as lava hits cold water and shatters.  When sucked into a jet engine,
it can heat up to about 1400 degree Celsius and re-melt.  And when
it cools again, it can stick onto the turbine blades.

<P>
This is not good.  In 1982, a British Airways Boeing 747 flew through 
an ash cloud created by a volcano in Indonesia.  All four engines cut 
out.  The plane descended from 11,000 meters to 3,700 meters before the
engines could be restarted.  <a href = "http://www.npr.org/templates/story/story.php?storyId=126069593">Whee!</a> 

<P> 
Here's a picture of the Eyjafjallajökull plume, taken yesterday by NASA's
&quot;Aqua&quot; satellite:

<P>
<div align = "center">
<a href = "http://rapidfire.sci.gsfc.nasa.gov/gallery/?2010105-0415/NorthAtlantic.A2010105.1135.2km.jpg">
<img width = "600" border = "2" src = "Eyjafjallajokull_ash_plume.jpg">
</a>
</div>

<P>
1) NASA, Ash plume from Eyjafjallajokull Volcano over the North Atlantic 
(afternoon overpass), <a href = "http://rapidfire.sci.gsfc.nasa.gov/gallery/?2010105-0415">http://rapidfire.sci.gsfc.nasa.gov/gallery/?2010105-0415</a>

<P>
Here's what the volcano looked like back in March:

<P>
<div align = "center">
<img width = "600" border = "2" src = "Eyjafjallajokull_bjarnit.jpg">
</div>
<P>

This photo was taken by someone named Bjarni T.  He has a great photo
gallery here:

<P>
2) Bjarni T, 2010 Eruptions of Eyjafjalljökull, 
<a href = "http://www.fotopedia.com/en/2010_eruptions_of_Eyjafjallaj%C3%B6kull/slideshow/sort/MostVotedFirst/status/default/photos">http://www.fotopedia.com/en/2010_eruptions_of_Eyjafjallaj%C3%B6kull/slideshow/sort/MostVotedFirst/status/default/photos</a>

<p>
And here's what the volcano looked like on April 15th:

<p>
<div align = "center">
<object width="400" height="325"><param name="movie" value="http://www.youtube-nocookie.com/v/f6D1M1AnBdY&hl=en_US&fs=1&rel=0&color1=0x402061&color2=0x9461ca&border=1"></param><param name="allowFullScreen" value="true"></param><param name="allowscriptaccess" value="always"></param><embed src="http://www.youtube-nocookie.com/v/f6D1M1AnBdY&hl=en_US&fs=1&rel=0&color1=0x402061&color2=0x9461ca&border=1" type="application/x-shockwave-flash" allowscriptaccess="always" allowfullscreen="true" width="400" height="325"></embed></object>
</div>
<p>

<P>
Starting around 1821, the same volcano erupted and put out ash for 
about 6 months.  What will it do this time?  Nobody seems to know.  
If it goes on long enough, will people invent some sort of ash filter
for jet engines?

<P>
Oh well.  Back to electrical circuits...

<P>
I want to explain the &quot;principle of minimum power&quot; and how
we can use it to understand electrical circuits built from linear
resistors.  In future Weeks this will lead us to some symplectic
geometry, complex analysis and loop groups.  But I want to start with
some very basic stuff!  I want to illustrate the principle of minimum
power by using it to solve two basic problems: resistors in series and
resistors in parallel.  But first I should work out the answers to
these problems using a more standard textbook approach - just in case
you haven't seen this stuff already.

<P>
In the textbook approach, we'll use Kirchoff's voltage and current
laws over and over again.  I explained these laws in &quot;<a href =
"week293.html">week293</A>&quot; and &quot;<a href =
"week294.html">week294</A>&quot; - so if necessary, you can either
review what I said there, or just nod and act like you understand what
I'm doing.

<P>
First, suppose we have two resistors &quot;in series&quot;.  This means they're
stuck together end to end, like this:
             
<PRE>
        |
        |
      -----
     | R<sub>1</sub>  |
      -----
        |
        |
      -----
     | R<sub>2</sub>  |
      -----
        |
        |
</PRE>

What happens when we put a voltage across this circuit?  How much
current will flow through?  

<P>
To answer this, fix the voltage across the whole circuit, say V.  
By Kirchoff's voltage law, this is the sum of the voltages across 
the individual resistors, say V<sub>1</sub> and V<sub>2</sub>:

<PRE>
        | 
        |
      -----
     | R<sub>1</sub>  | V<sub>1</sub>
      -----
        |                        V = V<sub>1</sub> + V<sub>2</sub>
        |        
      -----
     | R<sub>2</sub>  | V<sub>2</sub>
      -----
        |
        | 
</PRE>

Next let's think about the current flowing through each resistor.  By
Kirchhoff's current law, the current through the first resistor must
equal the current through the second one.  So, let's call this current
I in each case:

<PRE>
        | 
        |
      -----
   I | R<sub>1</sub>  | V<sub>1</sub>
      -----
        |                     V = V<sub>1</sub> + V<sub>2</sub>
        | 
      -----
   I | R<sub>2</sub>  | V<sub>2</sub>
      -----
        |
        |
</PRE>
Now, Ohm's law says that the voltage across a linear resistor equals
the current through it times its resistance.  Let's say our resistors
are linear.  So, we get:

<P>
I R<sub>1</sub> = V<sub>1</sub>

<P>
and

<P>
I R<sub>2</sub> = V<sub>2</sub>

<P>
Adding these two equations we get:

<P>
I (R<sub>1</sub> + R<sub>2</sub>) = V

<P>
This looks like Ohm's law again, but now for a resistor with
resistance R<sub>1</sub> + R<sub>2</sub>.  

<P>
The moral: two resistors in series act like a single resistor whose
resistance is the sum of theirs!

<P>
Next, suppose we have two resistors &quot;in parallel&quot;.  This
means they're stuck together side by side, like this:

<PRE>
             /\
            /  \
           /    \
          /      \
         /        \
        /          \
       /            \
     ----          ----
    | R<sub>1</sub> |        | R<sub>2</sub> |            
     ----          ----
       \            /
        \          /
         \        /
          \      /
           \    /
            \  /
             \/ 
</PRE>
What happens when we make some current flow through this circuit?  
What will the voltage across it be?

<P>
To answer this, fix the current through the whole circuit, say I.  
By Kirchoff's current law, this is the sum of the currents through
the individual resistors, say I<sub>1</sub> and I<sub>2</sub>:

<PRE>
             /\
            /  \
           /    \
          /      \
         /        \
        /          \
       /            \
     ----          ----
 I<sub>1</sub> | R<sub>1</sub> |     I<sub>2</sub> | R<sub>2</sub> |            I = I<sub>1</sub> + I<sub>2</sub>
     ----          ----
       \            /
        \          /
         \        /
          \      /
           \    /
            \  /
             \/ 
</PRE>
Next let's think about the voltage across each resistor.  By Kirchhoff's 
voltage law, the voltage across the first resistor must equal the voltage 
across the second one.  So, let's call this voltage V in each case:

<PRE>
             /\
            /  \
           /    \
          /      \
         /        \
        /          \
       /            \
     ----          ----
 I<sub>1</sub> | R<sub>1</sub> | V   I<sub>2</sub> | R<sub>2</sub> | V          I = I<sub>1</sub> + I<sub>2</sub>
     ----          ----
       \            /
        \          /
         \        /
          \      /
           \    /
            \  /
             \/ 
</PRE>
Now, Ohm's law says that the current through a linear resistor equals
the voltage across it divided by its resistance.  So, if our resistors
are linear, we get

<P>
I<sub>1</sub> = V / R<sub>1</sub>

<P>
and

<P>
I<sub>2</sub> = V / R<sub>2</sub>

<P>
Adding these two equations we get:

<P>
I = V (1/R<sub>1</sub> + 1/R<sub>2</sub>)

<P>
In our previous problem we were adding up resistances.  Now we're
adding up reciprocals of resistances.  Luckily, there's a name for 
the reciprocal of a resistance: it's called an &quot;admittance&quot;.  

<P>
The moral: two resistors in parallel act like a single resistor whose
admittance is the sum of theirs!

<P>
And there's also another moral.  If you compare this problem to the
previous one, you'll see that everything was almost exactly the same!
In fact, I repeated a lot of sentences almost word for word.  I just
switched certain concepts, which come in pairs:

<P>
<UL>
<LI>
current and voltage
</LI><LI>
series and parallel
</LI><LI>
resistance and admittance
</LI>
</UL>

<P>
In fact, switching concepts like this is an example of Poincar&eacute;
duality for electrical circuits, as mentioned in &quot;<a href =
"week291.html">week291</A>&quot;.

<P>
You may know Poincar&eacute; duality for graphs drawn on a sphere: you get a
new graph from an old one by:

<UL>
<LI>
  drawing a new vertex in the middle of each old face, 
</LI><LI>
  replacing each edge with a new one that crosses the old one, and
</LI><LI>
  drawing a new face centered at each old vertex.
</LI>
</UL>

<P>
This works fine for &quot;closed&quot; planar circuits - but for circuits with
input and output wires, like we've got here, we need Poincar&eacute; duality
for graphs drawn on a closed disk!  This is should probably be called
&quot;Poincar&eacute;-Lefschetz duality&quot;.

<P>
Instead of giving you a long-winded description of how this works, 
let me just illustrate it.  We start with two resistors in series.  
This is a graph with two edges and three vertices drawn on something 
that's topologically a closed disk.  Let's draw it on a rectangle:

<PRE>
 ......x...... 
 .     |     .      
 .     |     .      
 .     |     .
 .     o     .
 .     |     .
 .     |     .      
 .     |     .
 ......x...... 
</PRE>

The two dashed edges are the resistors.  The two vertices on the
boundary of the square, drawn as x's, are the &quot;input&quot; and
&quot;output&quot; vertices.  There's also a vertex in the interior of
the square, drawn as a little circle.

<P>
Now let's superimpose the Poincar&eacute; dual graph:

<PRE>
 ......x...... 
 .  ___|___  .      
 . /   |   \ .      
 ./    |    \.
 x     o     x
 .\    |    /.
 . \___|___/ .
 .     |     .      
 ......x...... 
</PRE>

This is a mess, so now let's remove the original graph:

<PRE>
 ............. 
 .  _______  .      
 . /       \ .      
 ./         \.
 x           x
 .\         /.
 . \_______/ .
 .           .      
 ............. 
</PRE>

This Poincar&eacute; dual graph shows two resistors in parallel!  There's an
&quot;input&quot; at left connected to an &quot;output&quot; at right
by two edges, each with a resistor on it.  In case you're wondering,
the difference between &quot;input&quot; and &quot;output&quot; is
purely conventional here.

<P>
Poincar&eacute; duality is cool.  But now let's solve the same problems -
resistors in series and resistors in parallel - using the
&quot;principle of least power&quot;.  Here's what the principle says.
Suppose we have any circuit made of resistors and we fix boundary
conditions at the wires leading in and out.  Then the circuit will do
whatever it takes to minimize the amount of power it uses - that is,
turns into heat.

<P>
What do I mean by &quot;boundary conditions&quot;?  Well, first of
all, I'm thinking of an electrical circuit as a graph with resistors
on its edges, and with some special vertices that we think of as
inputs and outputs:

<PRE>
        x           x
        |           |
        o-----------o
       / \          |
      /   \         |
     /     o--------o
    |     / \       | 
    |    /   \      |
    o---o-----o-----o
    |   |           |
    x   x           x
</PRE>
The inputs and outputs are marked as x's here.  I've drawn a planar
graph, but we could also have a nonplanar one, like this:

<PRE>
   x     x   x   x 
    \     \ /    |      
     \     \     | 
      \   / \    |
       \ /   \   | 
        o--------o 
       /       \ |
      /         \|
     o-----o-----o
     |     |     |
     x     x     x
</PRE>

(Poincar&eacute; duality works best for planar circuits, but I'm still
struggling to find its place in the grand scheme of things - for
example, how it permeates the big set of analogies between different
physical systems that I explained starting in &quot;<a href =
"week288.html">week288</A>&quot;.)

<P>
But what do I mean by &quot;boundary conditions&quot;?  Well, one sort of
boundary condition is to fix the &quot;electrostatic potential&quot; at the
input and output vertices of our graph.  Remember from last week that
the electrostatic potential is a function &phi; on the vertices of our
graph.  So, we'll specify the value of this function at the input and
output vertices.  Then we'll compute its values at all the other 
vertices using the principle of minimum power.

<P>
To do this, we need to remember some stuff from &quot;<a href =
"week293.html">week293</A>&quot; and &quot;<a href =
"week294.html">week294</A>&quot;.  First, for any edge

<PRE>
        e
  x --------&gt; y
</PRE>
the voltage across that edge, V(e), is given by

<P>
V(e) = &phi;(y) - &phi;(x)

<P>
Second, since we have a circuit made of linear resistors, the current
I(e) through that edge obeys Ohm's law:

<P>
V(e) = I(e) R(e)

<P>
where R(e) is the resistance.  

<P>
Third, the power consumed by that edge will be

<P>
P(e) = V(e) I(e) 
    
<P>
The principle of minimum power says: fix &phi; at the input and output 
vertices.  Then, to find &phi; at the other vertices, just minimize the
total power:

<P>
P = &sum;<sub>e</sub> P(e)  

<P>
Using all the equations I've lined up, we see that the total power
is indeed a function of &phi;, since:

<P>
P(e) = (&phi;(y) - &phi;(x))<sup>2</sup> / R(e)

<P>
The total power is a quadratic function in a bunch of variables, so
it's easy to minimize.

<P>
Let's actually do this for two resistors in series:

<PRE>
     &phi;<sub>0</sub> x
        |
        | R<sub>1</sub>
        |        
     &phi;<sub>1</sub> o
        |
        | R<sub>2</sub>
        |
     &phi;<sub>2</sub> x
</PRE>
We need to find &phi;<sub>1</sub> that minimizes the total power

<P>
P = (&phi;<sub>1</sub> - &phi;<sub>0</sub>)<sup>2</sup>/R<sub>1</sub>
+ (&phi;<sub>2</sub> - &phi;<sub>1</sub>)<sup>2</sup>/R<sub>2</sub>

<P>
So, we differentiate P with respect to &phi;<sub>1</sub> and set the derivative
to zero:

<P>
2(&phi;<sub>1</sub> - &phi;<sub>0</sub>)/R<sub>1</sub>  -  
2(&phi;<sub>2</sub> - &phi;<sub>1</sub>)/R<sub>2</sub>  =  0

<P>
This implies that

<P>
V<sub>1</sub> / R<sub>1</sub> = V<sub>2</sub> / R<sub>2</sub>

<P>
where V<sub>1</sub> and V<sub>2</sub> are the voltages across our two
resistors.

<P>
By Ohm's law, voltage divided by resistance is current.  So, we get

<P>
I<sub>1</sub> = I<sub>2</sub>

<P>
where I<sub>1</sub> and I<sub>2</sub> are the currents through our two
resistors.  Hey - the current flowing through the first resistor
equals the current flowing through the second one!  That's no surprise:
it's a special case of Kirchoff's current law!  But the cool part is
that we <i>derived</i> Kirchhoff's current law from the principle of
minimum power.  This works quite generally, not just in this baby
example.

<P>
Since the currents I<sub>1</sub> and I<sub>2</sub> are equal, let's
call them both I.  Then we're back to the textbook approach to this
problem.  Ohm's law says

<P>
I R<sub>1</sub> = V<sub>1</sub>

<P>
and

<P>
I R<sub>2</sub> = V<sub>2</sub>

<P>
Adding these equations, we see that when you put resistors in series,
their resistances add.

<P>
Okay, now let's try two resistors in parallel:

<PRE>
        x &phi;<sub>0</sub>
       / \
      /   \
     /     \
    /R<sub>1</sub>     \R<sub>2</sub>
    \       /
     \     /
      \   /
       \ /
        x &phi;<sub>1</sub>
</PRE>

This problem is oddly boring.  There are no vertices except the input
and the output, so the minimization problem is trivial!  If we fix the
potential at the input and output, we instantly know the voltages across 
the two resistors, and then using Ohm's law we get the currents.

<P>
Why was this problem more boring than two resistors in series?  Shouldn't 
they be very similar?  After all, they're Poincar&eacute; duals of each other!

<P>
Well, yeah.  But the problem is, we're not using the Poincar&eacute; dual
boundary conditions.  For the resistors in series we had a graph with 
a vertex in the middle:

<PRE>
 ......x...... 
 .     |     .      
 .     |     .      
 .     |     .
 .     o     .
 .     |     .
 .     |     .      
 .     |     .
 ......x...... 
</PRE>

For the resistors in parallel we have a graph with a face in the middle:


<PRE>
 ............. 
 .  _______  .      
 . /       \ .      
 ./         \.
 x           x
 .\         /.
 . \_______/ .
 .           .      
 ............. 
</PRE>

So, to treat the resistors in parallel in a Poincar&eacute; dual way,
we should use boundary conditions that involve faces rather than
vertices.  I talked about these faces back in &quot;<a href =
"week293.html">week293</A>&quot;: electrical engineers call them
&quot;meshes&quot;.  Each mesh has a current flowing around it.  So,
our boundary conditions should specify the current flowing around each
input or output mesh: that is, each mesh that touches the boundary of
our rectangle.  We should then find currents flowing around the
internal meshes that minimize the total power.  And in the process, we
should be able to derive Kirchhoff's <em>voltage</em> law.

<P>
All this could be further illuminated using the chain complex approach
I outlined in &quot;<a href = "week293.html">week293</A>&quot;.  Let
me just sketch how that goes.  We can associate a cochain complex to
our circuit:

<PRE>
       d           d
C<sup>0</sup> --------&gt; C<sup>1</sup> ---------&gt; C<sup>2</sup>
</PRE>

The electrostatic potential &phi; is a 0-cochain and the voltage

<P>
V = d&phi;

<P>
is a 1-cochain.  As we've seen, the total power is

<P>
P = &sum;<sub>e</sub> V(e)<sup>2</sup> / R(e)

<P>
We can write this in a slicker way using an inner product on
the space of 1-cochains:

<P>
P = &lt;V, V&gt;

<P>
The principle of minimum power says we should find the electrostatic 
potential &phi; that minimizes the total power subject to some boundary 
conditions.  So, we're trying to minimize

<P>
P = &lt;d&phi;, d&phi;&gt;

<P>
while holding &phi; fixed at some &quot;input and output
vertices&quot;.  If you know some mathematical physics you'll see this
is just a discretized version of the minimum principle that gives
Laplace's equation!

<P>
There's also a dual version of this whole story.  Our circuit
also gives a chain complex:

<PRE>
        &delta;           &delta;
C<sub>0</sub> &lt;-------- C<sub>1</sub> &lt;-------- C<sub>2</sub>
</PRE>

The mesh currents define a 2-chain J and the currents along edges
define a 1-chain

<P>
I = &delta;J

<P>
In these terms, the total power is

<P>
P = &sum;<sub>e</sub> R(e) I(e)<sup>2</sup> 

<P>
We can write this in a slicker way using an inner product on
the space of 1-chains:

<P>
P = &lt;I, I&gt;

<P>
In fact I already talked about this inner product in &quot;<a href =
"week293.html">week293</A>&quot;.

<P>
In these terms, the principle of minimum power says we should find the
mesh current that minimizes the total power subject to some boundary
conditions.  So, now we're trying to minimize

<P>
P = &lt;&delta;J, &delta;J&gt;

<P>
while holding J fixed along certain &quot;input and output meshes&quot;.  

<P>
In short, everything works the same way in the two dual formulations.
In fact, we can reinterpret our chain complex as a cochain complex 
just by turning it around!  This:

<PRE>
        &delta;           &delta;
C<sub>0</sub> &lt;-------- C<sub>1</sub> &lt;-------- C<sub>2</sub>
</PRE>

<P>
effortlessly becomes this:

<PRE>
        &delta;           &delta;
C<sub>2</sub> --------&gt; C<sub>1</sub> --------&gt; C<sub>0</sub>
</PRE>

<P>
And we didn't even need our graph to be planar!  The only point in
having the graph be planar is that this gives us a specific choice of
meshes.  Otherwise, we must choose them ourselves.

<P>
Finally, I want to mention an interesting book on nonequilibrium
thermodynamics.  The "principle of minimum power" is also known as the
"principle of least entropy production".  I'm very curious about this
principle and how it relates to the more familiar "principle of
least action" in classical mechanics.  This book seems to be pointing
towards a unification of the two:

<P>
3) Hans Christian &Ouml;ttinger, Beyond Equilibrium Thermodynamics, 
Wiley, 2005.

<P>
I thank Arnold Neumaier for pointing it out!  It considers a
fascinating generalization of Hamiltonian mechanics that applies to 
systems with dissipation: for example, electrical circuits with resistors,
or mechanical systems with friction. 

<P>
In ordinary Hamiltonian mechanics the space of states is a manifold
and time evolution is a flow on this manifold determined by a smooth
function called the Hamiltonian, which describes the <i>energy</i> of
any state.  In this generalization the space of states is still a
manifold, but now time evolution is determined by two smooth
functions: the energy and the <i>entropy!</i> In ordinary Hamiltonian
mechanics, energy is automatically conserved.  In this generalization
that's also true, but energy can go into the form of heat... and
entropy automatically <i>increases!</i>

<P>
Mathematically, the idea goes like this.  We start with a Poisson
manifold, but in addition to the skew-symmetric Poisson bracket {F,G}
of smooth functions on some manifold, we also have a symmetric
bilinear bracket [F,G] obeying the Leibniz law

<P>
[F,GH] = [F,G]H + G[F,H]

<P>
and this positivity condition:

<P>
[F,F] &ge; 0

<P>
The time evolution of any function is given by a generalization
of Hamilton's equations:

<P>
dF/dt = {H,F} + [S,F]

<P>
where H is a function called the &quot;energy&quot; or
&quot;Hamiltonian&quot;, and S is a function called the
entropy.   The first term on the right is the usual one.
The new second term describes dissipation: as we shall see,
it pushes the state towards increasing entropy.

<P>
If we require that

<P>
[H,F] = {S,F} = 0

<P>
for every function F, then we get conservation of energy, as usual
in Hamiltonian mechanics:

<P>
dH/dt = {H,H} + [S,H] = 0

<P>
But we also get the second law of thermodynamics:

<P>
dS/dt = {H,S} + [S,S] &ge; 0

<P>
Entropy always increases!

<P>
&Ouml;ttinger calls this framework "GENERIC" - an annoying acronym for
"General Equation for the NonEquilibrium Reversible-Irreversible
Coupling".  There are lots of papers about it.  But I'm wondering if
any geometers have looked into it!  

<P>
If we didn't need the equations [H,F] = {S,F} = 0, we could easily
get the necessary brackets starting with a K&auml;hler manifold.  The 
imaginary part of the K&auml;hler structure is a symplectic structure,
say &omega;, so we can define

<P>
{F,G} = &omega;(dF,dG)

<P>
as usual to get Poisson brackets.  The real part of the K&auml;hler structure
is a Riemannian structure, say g, so we can define

<P>
[F,G] = g(dF,dG)

<P>
This satisfies

<P>
[F,GH] = [F,G]H + F[G,H]
<P>
and 

<P>
[F,F] &ge; 0

<P>
Don't be fooled: this stuff is not rocket science.  In particular,
the inequality above has a simple meaning: when we move in the direction
of the gradient of F, the function F increases.  So adding the second
term to Hamilton's equations has the effect of pushing the system towards
increasing entropy.

<P>
Note that I'm being a tad unorthodox by letting &omega; and g eat
cotangent vectors instead of tangent vectors - but that's no big deal.
The big deal is this: if we start with a K&auml;hler manifold and
define brackets this way, we don't get [H,F] = 0 or {S,F} = 0 for all
functions F unless H and S are constant!  That's no good for
applications to physics.  To get around this problem, we would need to
consider some sort of <i>degenerate</i> K&auml;hler structure - one
where &omega; and g are degenerate bilinear forms on the cotangent
space.

<P>
Has anyone thought about such things?  They remind me a little of
&quot;Dirac structures&quot; and &quot;generalized complex
geometry&quot; - but I don't know enough about those subjects to know
if they're relevant here.

<P>
This GENERIC framework suggests that energy and entropy should be
viewed as two parts of a single entity - maybe even its real and
imaginary parts!  And that in turn reminds me of other strange 
things, like the idea of using complex-valued Hamiltonians 
to describe dissipative systems, or the idea of "inverse temperature 
as imaginary time".  I can't tell yet if there's a big
idea lurking here, or just a mess....

<P>
<HR>
<P>
<b>Addendum:</b> I thank Eric Forgy, Tom Leinster, Gunnar Magnusson and 
Esa Peuha for catching typos.  Also, Esa Peuha noticed that I was cutting
corners in my definition of "admittance" as the inverse of
"resistance".  Admittance is the inverse of resistance in circuits
made of linear resistors, which are the circuits I was talking about.
But he notes:

<blockquote>
   In Week 295, you claim that admittance is the inverse of resistance, 
   but that's not true; admittance is the inverse of impedance.  Of 
   course, resistance and impedance are the same thing for circuits 
   containing only resistors, but not in the presence of capacitors 
   and inductors.  Usually it's said that the inverse of resistance is 
   conductance (and the inverse of reactance is susceptance), but 
   that's not quite right: resistance and reactance are the real and 
   imaginary parts of impedance, and conductance and susceptance are 
   the real and imaginary parts of admittance, so resistance, reactance, 
   conductance and susceptance don't usually have physically meaningful 
   inverses.
</blockquote>

Someone pointed out that in the GENERIC formalism, we don't need

<p>
[H,F] = {S,F} = 0

<p>
for all functions F.  To derive the few results I describe, it's enough 
to have 

<p>
[H,S] = {S,H} = 0

<p>
It seems that &Ouml;ttinger assumes the stronger formulation but only uses 
the weaker one - see the text before equation (1.22) in his book.  This
changes the story considerably!

<p>
Eugene Lerman pointed out that everything I said about K&auml;hler manifolds
would work equally well for almost K&auml;hler manifolds: nothing I said
required that the complex structure relating the symplectic structure
and the Riemannian metric be integrable.  So, you could say I'm looking
to fill in the missing item in this analogy:

<p>
<div align = "center">
symplectic : Poisson :: almost K&auml;hler : ???
</div>

<p>
But, I'd be equally happy to hear from you if you know the missing
item in <i>this</i> analogy:

<p>
<div align = "center">
symplectic : Poisson :: K&auml;hler : ???
</div>

<p>
For more discussion, visit the <a href =
"http://golem.ph.utexas.edu/category/2010/04/this_weeks_finds_in_mathematic_56.html"><em>n</em>-Category
Caf&eacute;</a>.

<P>
<HR>
<P>

<em>I would rather discover a single fact, even a small one, than
debate the great issues at length without discovering anything new at
all.</em> - Galileo Galilei

<P>
